{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from pandas import read_csv\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, GRU\n",
    "from keras.models import model_from_json\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from IPython.display import SVG\n",
    "np.random.seed(123)  # for reproducibility\n",
    "\n",
    "# Extra part for memory management\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Make_LSTM_StepUp(train_index, val_index, test_index, state_condition='Stateless', shuffle='True', \n",
    "                     peak_hour=1, pred_hour=24, LSTM_hour=48, num_unit=240, num_epoch=1200, optimizer='Adam', loss_ftn='mean_squared_error'):\n",
    "    print(\"StepUp Output!\")\n",
    "    \n",
    "    len_PRED = pred_hour/peak_hour\n",
    "    len_LSTM = LSTM_hour/peak_hour\n",
    "    \n",
    "    len_PRED = int(len_PRED)\n",
    "    len_LSTM = int(len_LSTM)\n",
    "    \n",
    "    file_condition = str(peak_hour)+'hPEAK_'+str(pred_hour)+'hPrediction_'+str(LSTM_hour)+'hLSTM'\n",
    "    \n",
    "    if len_PRED <= 0:\n",
    "        print(\"You entered wrong predction hour or peak hour\")\n",
    "        sys.eixt()\n",
    "    if len_LSTM <= 0:\n",
    "        print(\"You entered wrong LSTM hour or peak hour\")\n",
    "        sys.eixt()\n",
    "       \n",
    "    allDataSet = read_csv('C:\\\\Users\\\\YKW\\\\GoogleDrive\\\\Study USB BackUp\\\\SEP\\\\GOES_LSTM_csv\\\\GOES15_pgt10_'+str(peak_hour)+'hPEAK_1hPrediction_'+str(LSTM_hour)+'hLSTM_log.csv', sep=',')\n",
    "    allDataSet_size = len(allDataSet)\n",
    "    \n",
    "    print(\"allDataSet: \", allDataSet.shape)\n",
    "    print(\"allDataSet_size: \", allDataSet_size)\n",
    "    print(\"allDataSet Type: \", type(allDataSet))\n",
    "    print(\"allDataSet_size Type: \", type(allDataSet_size))\n",
    "    \n",
    "    data_count = np.arange(len_LSTM)+1\n",
    "    data_count = data_count* (-1)\n",
    "    data_count = data_count +len_LSTM+1\n",
    "    data_title = []\n",
    "    for bi in range(len(data_count)):\n",
    "        data_title.append('b'+str(int(data_count[bi])))\n",
    "        \n",
    "    pred_count = np.arange(len_PRED)\n",
    "    pred_title = []\n",
    "    for ai in range(len(pred_count)):\n",
    "        pred_title.append('a'+str(int(pred_count[ai])))\n",
    "\n",
    "    #flux = allDataSet[['Year','Mon','Day','Hour','Min','b1','b2','b3','b4','b5','b6']].values\n",
    "    flux = allDataSet[data_title].values\n",
    "#    pred_flux = allDataSet[pred_title].values\n",
    "    now_flux = allDataSet['a0'].values\n",
    "    now_Year = allDataSet['Year'].values\n",
    "    now_Mon = allDataSet['Mon'].values\n",
    "    now_Day = allDataSet['Day'].values\n",
    "    now_Hour = allDataSet['Hour'].values\n",
    "    now_Min = allDataSet['Min'].values\n",
    "     \n",
    "    d_flux = np.array([flux])\n",
    "#    d_pred_flux = np.array([pred_flux])\n",
    "    d_now_flux = np.array([now_flux])\n",
    "    d_now_Year = np.array([now_Year])\n",
    "    d_now_Mon = np.array([now_Mon])\n",
    "    d_now_Day = np.array([now_Day])\n",
    "    d_now_Hour = np.array([now_Hour])\n",
    "    d_now_Min = np.array([now_Min])\n",
    "    \n",
    "    \n",
    "    #d_flux = np.array(d_flux).reshape(allDataSet_size, 1, 6)\n",
    "    #inputFluxShape = (1,6)    \n",
    "    d_flux = np.array(d_flux).reshape(allDataSet_size, len_LSTM,1) ### d_flux = np.array(d_flux).reshape(allDataSet_size, 1, len_LSTM)\n",
    "#    d_pred_flux = np.array(d_pred_flux).reshape(allDataSet_size, len_PRED)\n",
    "    d_now_flux = np.array([d_now_flux]).reshape(allDataSet_size, 1)\n",
    "    d_now_Year = np.array([d_now_Year]).reshape(allDataSet_size, 1)\n",
    "    d_now_Mon = np.array([d_now_Mon]).reshape(allDataSet_size, 1)\n",
    "    d_now_Day = np.array([d_now_Day]).reshape(allDataSet_size, 1)\n",
    "    d_now_Hour = np.array([d_now_Hour]).reshape(allDataSet_size, 1)\n",
    "    d_now_Min = np.array([d_now_Min]).reshape(allDataSet_size, 1)\n",
    "    \n",
    "\n",
    "    def M_LSTM_Stateful(shapes):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(num_unit,batch_input_shape=shapes, stateful=True, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "        model.summary()\n",
    "        return model\n",
    "            \n",
    "    def M_LSTM_Stateless(shapes):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(num_unit,input_shape=shapes, stateful=False, activation='relu'))  # model.add(LSTM(num_unit,input_shape=shapes, return_sequences=True, stateful=False))\n",
    "#        model.add(Dropout(0.3))\n",
    "#        model.add(LSTM(num_unit,input_shape=shapes, return_sequences=True, stateful=False))\n",
    "#        model.add(Dropout(0.3))\n",
    "#        model.add(LSTM(num_unit,input_shape=shapes, stateful=False))\n",
    "        model.add(Dense(1))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "\n",
    "    class PlotLosses(keras.callbacks.Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.i = 0\n",
    "            self.x = []\n",
    "            self.losses = []\n",
    "            self.val_losses = []\n",
    "\n",
    "            self.fig = plt.figure()\n",
    "\n",
    "            self.logs = []\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "            self.logs.append(logs)\n",
    "            self.x.append(self.i)\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.val_losses.append(logs.get('val_loss'))\n",
    "            self.i += 1\n",
    "            clear_output(wait=True)\n",
    "            plt.plot(self.x, self.losses, label=\"loss\")\n",
    "            plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show();\n",
    "            \n",
    "\n",
    "    if shuffle is 'True':\n",
    "        weightfileName = 'C:\\\\Users\\\\YKW\\\\GoogleDrive\\\\Study USB BackUp\\\\SEP\\\\GOES_LSTM_TEST_WEIGHT\\\\Layer1_shuffled_'+state_condition+'_StepUpOutput_LSTM_GOES15_pgt10_'+file_condition+'_ep'+str(num_epoch)+'_'+str(num_unit)+'units_WEIGHT'\n",
    "        testfileName = 'C:\\\\Users\\\\YKW\\\\GoogleDrive\\\\Study USB BackUp\\\\SEP\\\\GOES_LSTM_TEST_RESULT\\\\Layer1_shuffled_'+state_condition+'_StepUpOutput_TEST_'+file_condition+'_Proton_LSTM.csv'\n",
    "    else:\n",
    "        if shuffle is 'False':\n",
    "            weightfileName = 'C:\\\\Users\\\\YKW\\\\GoogleDrive\\\\Study USB BackUp\\\\SEP\\\\GOES_LSTM_TEST_WEIGHT\\\\Layer1_'+state_condition+'_StepUpOutput_LSTM_GOES15_pgt10_'+file_condition+'_ep'+str(num_epoch)+'_'+str(num_unit)+'units_WEIGHT'\n",
    "            testfileName = 'C:\\\\Users\\\\YKW\\\\GoogleDrive\\\\Study USB BackUp\\\\SEP\\\\GOES_LSTM_TEST_RESULT\\\\Layer1_'+state_condition+'_StepUpOutput_TEST_'+file_condition+'_Proton_LSTM.csv'\n",
    "            \n",
    "\n",
    "    plot_losses = PlotLosses()\n",
    "    \n",
    "    if state_condition is 'Stateful':\n",
    "        inputFluxShape = (1,len_LSTM, 1)\n",
    "        model = M_LSTM_Stateful(inputFluxShape)\n",
    "\n",
    "    else:\n",
    "        if state_condition is 'Stateless':\n",
    "            inputFluxShape = (len_LSTM,1)  #####  inputFluxShape = (1, len_LSTM)\n",
    "            model = M_LSTM_Stateless(inputFluxShape)\n",
    "        else:\n",
    "            print(\"You enterd wrong state_condition!\")\n",
    "            sys.exit()\n",
    " \n",
    "\n",
    "    val_index  = train_index + val_index\n",
    "    test_index = val_index + test_index\n",
    "    \n",
    "    trainX = d_flux[:train_index]\n",
    "    trainY = d_now_flux[:train_index]\n",
    "    trainYear = d_now_Year[:train_index]\n",
    "    trainMon = d_now_Mon[:train_index]\n",
    "    trainDay = d_now_Day[:train_index]\n",
    "    trainHour = d_now_Hour[:train_index]\n",
    "    trainMin = d_now_Min[:train_index]\n",
    "    \n",
    "    valX = d_flux[train_index:val_index]\n",
    "    valY = d_now_flux[train_index:val_index]\n",
    "    valYear = d_now_Year[train_index:val_index]\n",
    "    valMon = d_now_Mon[train_index:val_index]\n",
    "    valDay = d_now_Day[train_index:val_index]\n",
    "    valHour = d_now_Hour[train_index:val_index]\n",
    "    valMin = d_now_Min[train_index:val_index]\n",
    "    \n",
    "    testX = d_flux[val_index:test_index]\n",
    "    testY = d_now_flux[val_index:test_index]\n",
    "    testYear = d_now_Year[val_index:test_index]\n",
    "    testMon = d_now_Mon[val_index:test_index]\n",
    "    testDay = d_now_Day[val_index:test_index]\n",
    "    testHour = d_now_Hour[val_index:test_index]\n",
    "    testMin = d_now_Min[val_index:test_index]\n",
    "    \n",
    "    model.compile(loss=loss_ftn, optimizer=optimizer, metrics=['acc'])\n",
    "    \n",
    "    \n",
    "    if state_condition is 'Stateful':    \n",
    "        for epoch_idx in range(num_epoch):\n",
    "            if shuffle is 'True':\n",
    "                model.fit(trainX, trainY, epochs=1, batch_size=1, verbose=2, shuffle=True, validation_data=(valX, valY))\n",
    "                model.reset_states()\n",
    "            else:\n",
    "                if shuffle is 'False':\n",
    "                    model.fit(trainX, trainY, epochs=1, batch_size=1, verbose=2, shuffle=False, validation_data=(valX, valY))\n",
    "                    model.reset_states()\n",
    "            print(epoch_idx)\n",
    "    else:\n",
    "        if state_condition is 'Stateless': \n",
    "            if shuffle is 'True':\n",
    "                model.fit(trainX, trainY, epochs=num_epoch, batch_size=1400, verbose=2, shuffle=True, validation_data=(valX, valY))\n",
    "            else:\n",
    "                if shuffle is 'False':\n",
    "                    model.fit(trainX, trainY, epochs=num_epoch, batch_size=1400, verbose=2, shuffle=False, validation_data=(valX, valY))\n",
    "\n",
    "        \n",
    "    model_json = model.to_json()\n",
    "    with open(weightfileName + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(weightfileName + \".h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "    json_file = open(weightfileName + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    loaded_model.load_weights(weightfileName + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    loaded_model.compile(loss=loss_ftn, optimizer=optimizer, metrics=['acc'])\n",
    "    \n",
    "\n",
    "    #date_testY = allDataSet['date'].values[val_index:test_index]\n",
    "    f = open(testfileName, 'w')\n",
    "    f.write('StepUp Output_'+str(num_epoch)+'epochs_'+str(num_unit)+'units_'+optimizer + ' optimizer_' + loss_ftn+' loss function')\n",
    "    f.write('\\n')\n",
    "    f.write('Year' + ','+ 'Month' + ','+ 'Day' + ','+ 'Hour' + ','+ 'Min' + ',' + 'Ground')\n",
    "    for i in range(len_PRED):\n",
    "        f.write(','+ 'a'+str(i))\n",
    "    f.write('\\n')\n",
    "    \n",
    "        \n",
    "    for idx2 in range(0, len(testX)):\n",
    "        \n",
    "        \n",
    "        NowY = testY[idx2:idx2+1]\n",
    "\n",
    "        Year = testYear[idx2][0]\n",
    "        Mon = testMon[idx2][0]\n",
    "        Day = testDay[idx2][0]\n",
    "        Hour = testHour[idx2][0]\n",
    "        Min = testMin[idx2][0]\n",
    "\n",
    "        if idx2 != len(testX)-1:\n",
    "            Year2 = testYear[idx2+1][0]\n",
    "            Mon2 = testMon[idx2+1][0]\n",
    "            Day2 = testDay[idx2+1][0]\n",
    "            Hour2 = testHour[idx2+1][0]\n",
    "            Min2 = testMin[idx2+1][0]\n",
    "\n",
    "            pre_time = datetime.datetime(Year, Mon, Day, Hour, Min)\n",
    "            next_time = datetime.datetime(Year2, Mon2, Day2, Hour2, Min2)\n",
    "            Second_Term = peak_hour*3600   \n",
    "\n",
    "            Diff = next_time - pre_time\n",
    "            Diff = Diff.total_seconds()\n",
    "            Diff = Diff/Second_Term\n",
    "            Diff = Diff-1\n",
    "        \n",
    "        pred = []\n",
    "        pred_in = testX[idx2:idx2+1]  #    (1, 1, 96)            \n",
    "\n",
    "        for pred_n in range(len_PRED):\n",
    "            resultY = model.predict(pred_in)\n",
    "            pred.append(resultY)\n",
    "\n",
    "            pred_in = np.append(pred_in,resultY)\n",
    "            pred_in = pred_in[1:]\n",
    "            pred_in = pred_in.reshape(1, len_LSTM, 1)\n",
    "\n",
    "        date = str(Year) + ',' + str(Mon) + ',' + str(Day) + ',' + str(Hour) + ',' + str(Min)\n",
    "        f.write(date)\n",
    "        f.write(',' + str(NowY[0][0]))\n",
    "\n",
    "        for nwr in range(len_PRED):\n",
    "            f.write(',' + str(pred[nwr][0][0]))   \n",
    "\n",
    "        f.write('\\n')\n",
    "        pred_in = []\n",
    "        resultY = []\n",
    "        print(idx2)\n",
    "        \n",
    "        if idx2 != len(testX)-1:\n",
    "            if Diff >= 1:                            \n",
    "                Term = datetime.timedelta(seconds=Second_Term)\n",
    "                ddd = pre_time\n",
    "                for n_diff in range(int(Diff)):    \n",
    "                    ddd = ddd + Term\n",
    "                    #print(str(ddd.year) + ',' + str(ddd.month) + ',' + str(ddd.day) + ',' + str(ddd.hour) + ',' + str(ddd.minute) + '\\n')\n",
    "                    f.write(str(ddd.year) + ',' + str(ddd.month) + ',' + str(ddd.day) + ',' + str(ddd.hour) + ',' + str(ddd.minute))\n",
    "                    for wr in range(len_PRED+1):\n",
    "                        f.write(',' + str(-99999))\n",
    "                    f.write('\\n')\n",
    "\n",
    "    f.close()\n",
    "\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_LSTM_Onetime(train_index, val_index, test_index, state_condition='Stateless', shuffle='True', \n",
    "                      peak_hour=1, pred_hour=24, LSTM_hour=48, num_unit=240, num_epoch=1200, optimizer='Adam', loss_ftn='mean_squared_error'): \n",
    "    print(\"Onetime Output!\")\n",
    "\n",
    "    len_PRED = pred_hour/peak_hour\n",
    "    len_LSTM = LSTM_hour/peak_hour\n",
    "\n",
    "    len_PRED = int(len_PRED)\n",
    "    len_LSTM = int(len_LSTM)\n",
    "\n",
    "    file_condition = str(peak_hour)+'hPEAK_'+str(pred_hour)+'hPrediction_'+str(LSTM_hour)+'hLSTM'\n",
    "\n",
    "    if len_PRED <= 0:\n",
    "        print(\"You entered wrong predction hour or peak hour\")\n",
    "        sys.eixt()\n",
    "    if len_LSTM <= 0:\n",
    "        print(\"You entered wrong LSTM hour or peak hour\")\n",
    "        sys.eixt()\n",
    "\n",
    "    allDataSet = read_csv('C:\\\\Users\\\\YKW\\\\GoogleDrive\\\\Study USB BackUp\\\\SEP\\\\GOES_LSTM_csv\\\\GOES15_pgt10_'+file_condition+'_log.csv', sep=',')\n",
    "    allDataSet_size = len(allDataSet)\n",
    "\n",
    "    print(\"allDataSet: \", allDataSet.shape)\n",
    "    print(\"allDataSet_size: \", allDataSet_size)\n",
    "    print(\"allDataSet Type: \", type(allDataSet))\n",
    "    print(\"allDataSet_size Type: \", type(allDataSet_size))\n",
    "\n",
    "    data_count = np.arange(len_LSTM)+1\n",
    "    data_count = data_count* (-1)\n",
    "    data_count = data_count +len_LSTM+1\n",
    "    data_title = []\n",
    "    for bi in range(len(data_count)):\n",
    "        data_title.append('b'+str(int(data_count[bi])))\n",
    "\n",
    "    pred_count = np.arange(len_PRED)\n",
    "    pred_title = []\n",
    "    for ai in range(len(pred_count)):\n",
    "        pred_title.append('a'+str(int(pred_count[ai])))\n",
    "\n",
    "    #flux = allDataSet[['Year','Mon','Day','Hour','Min','b1','b2','b3','b4','b5','b6']].values\n",
    "    flux = allDataSet[data_title].values\n",
    "    pred_flux = allDataSet[pred_title].values\n",
    "    now_flux = allDataSet['a0'].values\n",
    "    now_Year = allDataSet['Year'].values\n",
    "    now_Mon = allDataSet['Mon'].values\n",
    "    now_Day = allDataSet['Day'].values\n",
    "    now_Hour = allDataSet['Hour'].values\n",
    "    now_Min = allDataSet['Min'].values\n",
    "\n",
    "    d_flux = np.array([flux])\n",
    "    d_pred_flux = np.array([pred_flux])\n",
    "    d_now_flux = np.array([now_flux])\n",
    "    d_now_Year = np.array([now_Year])\n",
    "    d_now_Mon = np.array([now_Mon])\n",
    "    d_now_Day = np.array([now_Day])\n",
    "    d_now_Hour = np.array([now_Hour])\n",
    "    d_now_Min = np.array([now_Min])\n",
    "\n",
    "\n",
    "    #d_flux = np.array(d_flux).reshape(allDataSet_size, 1, 6)\n",
    "    #inputFluxShape = (1,6)    \n",
    "    d_flux = np.array(d_flux).reshape(allDataSet_size, len_LSTM,1) #### d_flux = np.array(d_flux).reshape(allDataSet_size, 1, len_LSTM)\n",
    "    d_pred_flux = np.array(d_pred_flux).reshape(allDataSet_size, len_PRED)\n",
    "    d_now_flux = np.array([d_now_flux]).reshape(allDataSet_size, 1)\n",
    "    d_now_Year = np.array([d_now_Year]).reshape(allDataSet_size, 1)\n",
    "    d_now_Mon = np.array([d_now_Mon]).reshape(allDataSet_size, 1)\n",
    "    d_now_Day = np.array([d_now_Day]).reshape(allDataSet_size, 1)\n",
    "    d_now_Hour = np.array([d_now_Hour]).reshape(allDataSet_size, 1)\n",
    "    d_now_Min = np.array([d_now_Min]).reshape(allDataSet_size, 1)\n",
    "\n",
    "\n",
    "    def M_LSTM_Stateful(shapes):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(num_unit,batch_input_shape=shapes, stateful=True, activation='relu'))\n",
    "        model.add(Dense(len_PRED))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def M_LSTM_Stateless(shapes):\n",
    "        model = Sequential()\n",
    "        model.add(GRU(num_unit,input_shape=shapes, stateful=False, activation='relu'))   # model.add(LSTM(num_unit,input_shape=shapes, return_sequences = True, stateful=False))  \n",
    "#        model.add(Dropout(0.3))\n",
    "#        model.add(LSTM(num_unit,input_shape=shapes, return_sequences = True, stateful=False))\n",
    "#        model.add(Dropout(0.3))\n",
    "#        model.add(LSTM(num_unit,input_shape=shapes, stateful=False))\n",
    "        model.add(Dense(len_PRED))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    class PlotLosses(keras.callbacks.Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.i = 0\n",
    "            self.x = []\n",
    "            self.losses = []\n",
    "            self.val_losses = []\n",
    "\n",
    "            self.fig = plt.figure()\n",
    "\n",
    "            self.logs = []\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "            self.logs.append(logs)\n",
    "            self.x.append(self.i)\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.val_losses.append(logs.get('val_loss'))\n",
    "            self.i += 1\n",
    "            clear_output(wait=True)\n",
    "            plt.plot(self.x, self.losses, label=\"loss\")\n",
    "            plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show();\n",
    "\n",
    "\n",
    "    if shuffle is 'True':\n",
    "        weightfileName = 'C:\\\\Users\\\\YKW\\\\GoogleDrive\\\\Study\\\\SEP\\\\GOES_LSTM_TEST_WEIGHT\\\\Layer1_shuffled_'+state_condition+'_OnetimeOutput_LSTM_GOES15_pgt10_'+file_condition+'_ep'+str(num_epoch)+'_'+str(num_unit)+'units_WEIGHT'\n",
    "        testfileName = 'C:\\\\Users\\\\YKW\\\\GoogleDrive\\\\Study\\\\SEP\\\\GOES_LSTM_TEST_RESULT\\\\Layer1_shuffled_'+state_condition+'_OnetimeOutput_TEST_'+file_condition+'_ep'+str(num_epoch)+'_'+str(num_unit)+'units'+'_Proton_LSTM.csv'\n",
    "    else:\n",
    "        if shuffle is 'False':\n",
    "            weightfileName = 'C:\\\\Users\\\\YKW\\\\GoogleDrive\\\\Study\\\\SEP\\\\GOES_LSTM_TEST_WEIGHT\\\\Layer1_'+state_condition+'_OnetimeOutput_LSTM_GOES15_pgt10_'+file_condition+'_ep'+str(num_epoch)+'_'+str(num_unit)+'units_WEIGHT'\n",
    "            testfileName = 'C:\\\\Users\\\\YKW\\\\GoogleDrive\\\\Study\\\\SEP\\\\GOES_LSTM_TEST_RESULT\\\\Layer1_'+state_condition+'_OnetimeOutput_TEST_'+file_condition+'_ep'+str(num_epoch)+'_'+str(num_unit)+'units'+'_Proton_LSTM.csv'\n",
    "\n",
    "    plot_losses = PlotLosses()\n",
    "\n",
    "    if state_condition is 'Stateful':\n",
    "        inputFluxShape = (1,len_LSTM, 1)\n",
    "        model = M_LSTM_Stateful(inputFluxShape)\n",
    "\n",
    "    else:\n",
    "        if state_condition is 'Stateless':\n",
    "            inputFluxShape = (len_LSTM,1)  #####  inputFluxShape = (1, len_LSTM)\n",
    "            model = M_LSTM_Stateless(inputFluxShape)\n",
    "        else:\n",
    "            print(\"You enterd wrong state_condition!\")\n",
    "            sys.exit()\n",
    "\n",
    "    \n",
    "    SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "    \n",
    "    sys.exit()\n",
    "    \n",
    "\n",
    "    val_index  = train_index + val_index\n",
    "    test_index = val_index + test_index\n",
    "\n",
    "    trainX = d_flux[:train_index]\n",
    "    trainY = d_pred_flux[:train_index]\n",
    "    trainYear = d_now_Year[:train_index]\n",
    "    trainMon = d_now_Mon[:train_index]\n",
    "    trainDay = d_now_Day[:train_index]\n",
    "    trainHour = d_now_Hour[:train_index]\n",
    "    trainMin = d_now_Min[:train_index]\n",
    "\n",
    "    valX = d_flux[train_index:val_index]\n",
    "    valY = d_pred_flux[train_index:val_index]\n",
    "    valYear = d_now_Year[train_index:val_index]\n",
    "    valMon = d_now_Mon[train_index:val_index]\n",
    "    valDay = d_now_Day[train_index:val_index]\n",
    "    valHour = d_now_Hour[train_index:val_index]\n",
    "    valMin = d_now_Min[train_index:val_index]\n",
    "\n",
    "    testX = d_flux[val_index:test_index]\n",
    "    testY = d_pred_flux[val_index:test_index]\n",
    "    testNowY = d_now_flux[val_index:test_index]\n",
    "    testYear = d_now_Year[val_index:test_index]\n",
    "    testMon = d_now_Mon[val_index:test_index]\n",
    "    testDay = d_now_Day[val_index:test_index]\n",
    "    testHour = d_now_Hour[val_index:test_index]\n",
    "    testMin = d_now_Min[val_index:test_index]\n",
    "\n",
    "    model.compile(loss=loss_ftn, optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "\n",
    "    if state_condition is 'Stateful':    \n",
    "        for epoch_idx in range(num_epoch):\n",
    "            if shuffle is 'True':\n",
    "                model.fit(trainX, trainY, epochs=1, batch_size=1, verbose=2, shuffle=True, validation_data=(valX, valY))\n",
    "                model.reset_states()\n",
    "            else:\n",
    "                if shuffle is 'False':\n",
    "                    model.fit(trainX, trainY, epochs=1, batch_size=1, verbose=2, shuffle=False, validation_data=(valX, valY))\n",
    "                    model.reset_states()\n",
    "            print(epoch_idx)\n",
    "    else:\n",
    "        if state_condition is 'Stateless': \n",
    "            if shuffle is 'True':\n",
    "                model.fit(trainX, trainY, epochs=num_epoch, batch_size=1400, verbose=2, shuffle=True, validation_data=(valX, valY))\n",
    "            else:\n",
    "                if shuffle is 'False':\n",
    "                    model.fit(trainX, trainY, epochs=num_epoch, batch_size=1400, verbose=2, shuffle=False, validation_data=(valX, valY))\n",
    "\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(weightfileName + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(weightfileName + \".h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "    json_file = open(weightfileName + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    loaded_model.load_weights(weightfileName + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    loaded_model.compile(loss=loss_ftn, optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "\n",
    "    #date_testY = allDataSet['date'].values[val_index:test_index]\n",
    "    f = open(testfileName, 'w')\n",
    "    f.write('Onetime Output_'+str(num_epoch)+'epochs_'+str(num_unit)+'units_'+optimizer + ' optimizer_' + loss_ftn+' loss function')\n",
    "    f.write('\\n')\n",
    "    f.write('Year' + ','+ 'Month' + ','+ 'Day' + ','+ 'Hour' + ','+ 'Min' + ',' + 'Ground')\n",
    "    for i in range(len_PRED):\n",
    "        f.write(','+ 'a'+str(i))\n",
    "    f.write('\\n')\n",
    "\n",
    "    for idx2 in range(0, len(testX)):\n",
    "\n",
    "        NowY = testNowY[idx2:idx2+1]\n",
    "\n",
    "        Year = testYear[idx2][0]\n",
    "        Mon = testMon[idx2][0]\n",
    "        Day = testDay[idx2][0]\n",
    "        Hour = testHour[idx2][0]\n",
    "        Min = testMin[idx2][0]\n",
    "\n",
    "        if idx2 != len(testX)-1:\n",
    "            Year2 = testYear[idx2+1][0]\n",
    "            Mon2 = testMon[idx2+1][0]\n",
    "            Day2 = testDay[idx2+1][0]\n",
    "            Hour2 = testHour[idx2+1][0]\n",
    "            Min2 = testMin[idx2+1][0]\n",
    "\n",
    "            pre_time = datetime.datetime(Year, Mon, Day, Hour, Min)\n",
    "            next_time = datetime.datetime(Year2, Mon2, Day2, Hour2, Min2)\n",
    "            Second_Term = peak_hour*3600   \n",
    "\n",
    "            Diff = next_time - pre_time\n",
    "            Diff = Diff.total_seconds()\n",
    "            Diff = Diff/Second_Term\n",
    "            Diff = Diff-1\n",
    "\n",
    "        pred = []\n",
    "        pred_in = testX[idx2:idx2+1]      \n",
    "\n",
    "        resultY = model.predict(pred_in)\n",
    "\n",
    "        date = str(Year) + ',' + str(Mon) + ',' + str(Day) + ',' + str(Hour) + ',' + str(Min)\n",
    "\n",
    "        f.write(date)\n",
    "        f.write(',' + str(NowY[0][0]))\n",
    "\n",
    "        for nwr in range(len_PRED):\n",
    "            f.write(',' + str(resultY[0][nwr]))   \n",
    "\n",
    "        f.write('\\n')\n",
    "        pred_in = []\n",
    "        resultY = []\n",
    "        print(idx2)\n",
    "\n",
    "        if idx2 != len(testX)-1:\n",
    "            if Diff >= 1:                            \n",
    "                Term = datetime.timedelta(seconds=Second_Term)\n",
    "                ddd = pre_time\n",
    "                for n_diff in range(int(Diff)):    \n",
    "                    ddd = ddd + Term\n",
    "                    #print(str(ddd.year) + ',' + str(ddd.month) + ',' + str(ddd.day) + ',' + str(ddd.hour) + ',' + str(ddd.minute) + '\\n')\n",
    "                    f.write(str(ddd.year) + ',' + str(ddd.month) + ',' + str(ddd.day) + ',' + str(ddd.hour) + ',' + str(ddd.minute))\n",
    "                    for wr in range(len_PRED+1):\n",
    "                        f.write(',' + str(-99999))\n",
    "                    f.write('\\n')\n",
    "\n",
    "    f.close()\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_LSTM_Onetime_load(train_index, val_index, test_index, state_condition='Stateless', shuffle='True', \n",
    "                      peak_hour=1, pred_hour=24, LSTM_hour=48, num_unit=240, num_epoch=1200, optimizer='Adam', loss_ftn='mean_squared_error'): \n",
    "    print(\"Onetime Output!\")\n",
    "\n",
    "    len_PRED = pred_hour/peak_hour\n",
    "    len_LSTM = LSTM_hour/peak_hour\n",
    "\n",
    "    len_PRED = int(len_PRED)\n",
    "    len_LSTM = int(len_LSTM)\n",
    "\n",
    "    file_condition = str(peak_hour)+'hPEAK_'+str(pred_hour)+'hPrediction_'+str(LSTM_hour)+'hLSTM'\n",
    "\n",
    "    if len_PRED <= 0:\n",
    "        print(\"You entered wrong predction hour or peak hour\")\n",
    "        sys.eixt()\n",
    "    if len_LSTM <= 0:\n",
    "        print(\"You entered wrong LSTM hour or peak hour\")\n",
    "        sys.eixt()\n",
    "\n",
    "    allDataSet = read_csv('C:\\\\Users\\\\YKW\\\\GoogleDrive\\\\Study USB BackUp\\\\SEP\\\\GOES_LSTM_csv\\\\GOES15_pgt10_'+file_condition+'_log.csv', sep=',')\n",
    "    allDataSet_size = len(allDataSet)\n",
    "\n",
    "    print(\"allDataSet: \", allDataSet.shape)\n",
    "    print(\"allDataSet_size: \", allDataSet_size)\n",
    "    print(\"allDataSet Type: \", type(allDataSet))\n",
    "    print(\"allDataSet_size Type: \", type(allDataSet_size))\n",
    "\n",
    "    data_count = np.arange(len_LSTM)+1\n",
    "    data_count = data_count* (-1)\n",
    "    data_count = data_count +len_LSTM+1\n",
    "    data_title = []\n",
    "    for bi in range(len(data_count)):\n",
    "        data_title.append('b'+str(int(data_count[bi])))\n",
    "\n",
    "    pred_count = np.arange(len_PRED)\n",
    "    pred_title = []\n",
    "    for ai in range(len(pred_count)):\n",
    "        pred_title.append('a'+str(int(pred_count[ai])))\n",
    "\n",
    "    #flux = allDataSet[['Year','Mon','Day','Hour','Min','b1','b2','b3','b4','b5','b6']].values\n",
    "    flux = allDataSet[data_title].values\n",
    "    pred_flux = allDataSet[pred_title].values\n",
    "    now_flux = allDataSet['a0'].values\n",
    "    now_Year = allDataSet['Year'].values\n",
    "    now_Mon = allDataSet['Mon'].values\n",
    "    now_Day = allDataSet['Day'].values\n",
    "    now_Hour = allDataSet['Hour'].values\n",
    "    now_Min = allDataSet['Min'].values\n",
    "\n",
    "    d_flux = np.array([flux])\n",
    "    d_pred_flux = np.array([pred_flux])\n",
    "    d_now_flux = np.array([now_flux])\n",
    "    d_now_Year = np.array([now_Year])\n",
    "    d_now_Mon = np.array([now_Mon])\n",
    "    d_now_Day = np.array([now_Day])\n",
    "    d_now_Hour = np.array([now_Hour])\n",
    "    d_now_Min = np.array([now_Min])\n",
    "\n",
    "\n",
    "    #d_flux = np.array(d_flux).reshape(allDataSet_size, 1, 6)\n",
    "    #inputFluxShape = (1,6)    \n",
    "    d_flux = np.array(d_flux).reshape(allDataSet_size, len_LSTM,1) #### d_flux = np.array(d_flux).reshape(allDataSet_size, 1, len_LSTM)\n",
    "    d_pred_flux = np.array(d_pred_flux).reshape(allDataSet_size, len_PRED)\n",
    "    d_now_flux = np.array([d_now_flux]).reshape(allDataSet_size, 1)\n",
    "    d_now_Year = np.array([d_now_Year]).reshape(allDataSet_size, 1)\n",
    "    d_now_Mon = np.array([d_now_Mon]).reshape(allDataSet_size, 1)\n",
    "    d_now_Day = np.array([d_now_Day]).reshape(allDataSet_size, 1)\n",
    "    d_now_Hour = np.array([d_now_Hour]).reshape(allDataSet_size, 1)\n",
    "    d_now_Min = np.array([d_now_Min]).reshape(allDataSet_size, 1)\n",
    "\n",
    "\n",
    "    def M_LSTM_Stateful(shapes):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(num_unit,batch_input_shape=shapes, stateful=True, activation='relu'))\n",
    "        model.add(Dense(len_PRED))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def M_LSTM_Stateless(shapes):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(num_unit,input_shape=shapes, stateful=False, activation='relu'))   # model.add(LSTM(num_unit,input_shape=shapes, return_sequences = True, stateful=False))  \n",
    "#        model.add(Dropout(0.3))\n",
    "#        model.add(LSTM(num_unit,input_shape=shapes, return_sequences = True, stateful=False))\n",
    "#        model.add(Dropout(0.3))\n",
    "#        model.add(LSTM(num_unit,input_shape=shapes, stateful=False))\n",
    "        model.add(Dense(len_PRED))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    class PlotLosses(keras.callbacks.Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.i = 0\n",
    "            self.x = []\n",
    "            self.losses = []\n",
    "            self.val_losses = []\n",
    "\n",
    "            self.fig = plt.figure()\n",
    "\n",
    "            self.logs = []\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "            self.logs.append(logs)\n",
    "            self.x.append(self.i)\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.val_losses.append(logs.get('val_loss'))\n",
    "            self.i += 1\n",
    "            clear_output(wait=True)\n",
    "            plt.plot(self.x, self.losses, label=\"loss\")\n",
    "            plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show();\n",
    "\n",
    "\n",
    "    if shuffle is 'True':\n",
    "        weightfileName = 'C:\\\\Users\\\\YKW\\\\GoogleDrive\\\\Study USB BackUp\\\\SEP\\\\GOES_LSTM_TEST_WEIGHT\\\\Layer1_shuffled_'+state_condition+'_OnetimeOutput_LSTM_GOES15_pgt10_'+file_condition+'_ep'+str(num_epoch)+'_'+str(num_unit)+'units_WEIGHT'\n",
    "        testfileName = 'C:\\\\Users\\\\YKW\\\\GoogleDrive\\\\Study USB BackUp\\\\SEP\\\\GOES_LSTM_TEST_RESULT\\\\Layer1_shuffled_'+state_condition+'_OnetimeOutput_TEST_'+file_condition+'_ep'+str(num_epoch)+'_'+str(num_unit)+'units'+'_Proton_LSTM.csv'\n",
    "    else:\n",
    "        if shuffle is 'False':\n",
    "            weightfileName = 'C:\\\\Users\\\\YKW\\\\GoogleDrive\\\\Study USB BackUp\\\\SEP\\\\GOES_LSTM_TEST_WEIGHT\\\\Layer1_'+state_condition+'_OnetimeOutput_LSTM_GOES15_pgt10_'+file_condition+'_ep'+str(num_epoch)+'_'+str(num_unit)+'units_WEIGHT'\n",
    "            testfileName = 'C:\\\\Users\\\\YKW\\\\GoogleDrive\\\\Study USB BackUp\\\\SEP\\\\GOES_LSTM_TEST_RESULT\\\\Layer1_'+state_condition+'_OnetimeOutput_TEST_'+file_condition+'_ep'+str(num_epoch)+'_'+str(num_unit)+'units'+'_Proton_LSTM.csv'\n",
    "\n",
    "    plot_losses = PlotLosses()\n",
    "\n",
    "    if state_condition is 'Stateful':\n",
    "        inputFluxShape = (1,len_LSTM, 1)\n",
    "        model = M_LSTM_Stateful(inputFluxShape)\n",
    "\n",
    "    else:\n",
    "        if state_condition is 'Stateless':\n",
    "            inputFluxShape = (len_LSTM,1)  #####  inputFluxShape = (1, len_LSTM)\n",
    "            model = M_LSTM_Stateless(inputFluxShape)\n",
    "        else:\n",
    "            print(\"You enterd wrong state_condition!\")\n",
    "            sys.exit()\n",
    "\n",
    "\n",
    "    val_index  = train_index + val_index\n",
    "    test_index = val_index + test_index\n",
    "\n",
    "    trainX = d_flux[:train_index]\n",
    "    trainY = d_pred_flux[:train_index]\n",
    "    trainYear = d_now_Year[:train_index]\n",
    "    trainMon = d_now_Mon[:train_index]\n",
    "    trainDay = d_now_Day[:train_index]\n",
    "    trainHour = d_now_Hour[:train_index]\n",
    "    trainMin = d_now_Min[:train_index]\n",
    "\n",
    "    valX = d_flux[train_index:val_index]\n",
    "    valY = d_pred_flux[train_index:val_index]\n",
    "    valYear = d_now_Year[train_index:val_index]\n",
    "    valMon = d_now_Mon[train_index:val_index]\n",
    "    valDay = d_now_Day[train_index:val_index]\n",
    "    valHour = d_now_Hour[train_index:val_index]\n",
    "    valMin = d_now_Min[train_index:val_index]\n",
    "\n",
    "    testX = d_flux[val_index:test_index]\n",
    "    testY = d_pred_flux[val_index:test_index]\n",
    "    testNowY = d_now_flux[val_index:test_index]\n",
    "    testYear = d_now_Year[val_index:test_index]\n",
    "    testMon = d_now_Mon[val_index:test_index]\n",
    "    testDay = d_now_Day[val_index:test_index]\n",
    "    testHour = d_now_Hour[val_index:test_index]\n",
    "    testMin = d_now_Min[val_index:test_index]\n",
    "\n",
    "    model.compile(loss=loss_ftn, optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "\n",
    "#    if state_condition is 'Stateful':    \n",
    "#        for epoch_idx in range(num_epoch):\n",
    "#            if shuffle is 'True':\n",
    "#                model.fit(trainX, trainY, epochs=1, batch_size=1, verbose=2, shuffle=True, validation_data=(valX, valY))\n",
    "#                model.reset_states()\n",
    "#            else:\n",
    "#                if shuffle is 'False':\n",
    "#                    model.fit(trainX, trainY, epochs=1, batch_size=1, verbose=2, shuffle=False, validation_data=(valX, valY))\n",
    "#                    model.reset_states()\n",
    "#            print(epoch_idx)\n",
    "#    else:\n",
    "#        if state_condition is 'Stateless': \n",
    "#            if shuffle is 'True':\n",
    "#                model.fit(trainX, trainY, epochs=num_epoch, batch_size=1400, verbose=2, shuffle=True, validation_data=(valX, valY))\n",
    "#            else:\n",
    "#                if shuffle is 'False':\n",
    "#                    model.fit(trainX, trainY, epochs=num_epoch, batch_size=1400, verbose=2, shuffle=False, validation_data=(valX, valY))\n",
    "#    model_json = model.to_json()\n",
    "#    with open(weightfileName + \".json\", \"w\") as json_file:\n",
    "#        json_file.write(model_json)\n",
    "#    model.save_weights(weightfileName + \".h5\")\n",
    "#    print(\"Saved model to disk\")\n",
    "\n",
    "    json_file = open(weightfileName + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    loaded_model.load_weights(weightfileName + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    loaded_model.compile(loss=loss_ftn, optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "\n",
    "    #date_testY = allDataSet['date'].values[val_index:test_index]\n",
    "    f = open(testfileName, 'w')\n",
    "    f.write('Onetime Output_'+str(num_epoch)+'epochs_'+str(num_unit)+'units_'+optimizer + ' optimizer_' + loss_ftn+' loss function')\n",
    "    f.write('\\n')\n",
    "    f.write('Year' + ','+ 'Month' + ','+ 'Day' + ','+ 'Hour' + ','+ 'Min' + ',' + 'Ground')\n",
    "    for i in range(len_PRED):\n",
    "        f.write(','+ 'a'+str(i))\n",
    "    f.write('\\n')\n",
    "\n",
    "    for idx2 in range(0, len(testX)):\n",
    "\n",
    "        NowY = testNowY[idx2:idx2+1]\n",
    "\n",
    "        Year = testYear[idx2][0]\n",
    "        Mon = testMon[idx2][0]\n",
    "        Day = testDay[idx2][0]\n",
    "        Hour = testHour[idx2][0]\n",
    "        Min = testMin[idx2][0]\n",
    "\n",
    "        if idx2 != len(testX)-1:\n",
    "            Year2 = testYear[idx2+1][0]\n",
    "            Mon2 = testMon[idx2+1][0]\n",
    "            Day2 = testDay[idx2+1][0]\n",
    "            Hour2 = testHour[idx2+1][0]\n",
    "            Min2 = testMin[idx2+1][0]\n",
    "\n",
    "            pre_time = datetime.datetime(Year, Mon, Day, Hour, Min)\n",
    "            next_time = datetime.datetime(Year2, Mon2, Day2, Hour2, Min2)\n",
    "            Second_Term = peak_hour*3600   \n",
    "\n",
    "            Diff = next_time - pre_time\n",
    "            Diff = Diff.total_seconds()\n",
    "            Diff = Diff/Second_Term\n",
    "            Diff = Diff-1\n",
    "\n",
    "        pred = []\n",
    "        pred_in = testX[idx2:idx2+1]      \n",
    "\n",
    "        resultY = loaded_model.predict(pred_in)\n",
    "\n",
    "        date = str(Year) + ',' + str(Mon) + ',' + str(Day) + ',' + str(Hour) + ',' + str(Min)\n",
    "\n",
    "        f.write(date)\n",
    "        f.write(',' + str(NowY[0][0]))\n",
    "\n",
    "        for nwr in range(len_PRED):\n",
    "            f.write(',' + str(resultY[0][nwr]))   \n",
    "\n",
    "        f.write('\\n')\n",
    "        pred_in = []\n",
    "        resultY = []\n",
    "        print(idx2)\n",
    "\n",
    "        if idx2 != len(testX)-1:\n",
    "            if Diff >= 1:                            \n",
    "                Term = datetime.timedelta(seconds=Second_Term)\n",
    "                ddd = pre_time\n",
    "                for n_diff in range(int(Diff)):    \n",
    "                    ddd = ddd + Term\n",
    "                    #print(str(ddd.year) + ',' + str(ddd.month) + ',' + str(ddd.day) + ',' + str(ddd.hour) + ',' + str(ddd.minute) + '\\n')\n",
    "                    f.write(str(ddd.year) + ',' + str(ddd.month) + ',' + str(ddd.day) + ',' + str(ddd.hour) + ',' + str(ddd.minute))\n",
    "                    for wr in range(len_PRED+1):\n",
    "                        f.write(',' + str(-99999))\n",
    "                    f.write('\\n')\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 1200\n",
    "num_unit = 240\n",
    "\n",
    "#Make_LSTM_StepUp(42526, 8605, 8325, state_condition='Stateless', shuffle='True', peak_hour=1, pred_hour=24, LSTM_hour=24, \n",
    "#                 num_unit=num_unit, num_epoch=num_epoch, optimizer=\"Adam\",loss_ftn='mean_squared_error')\n",
    "#Make_LSTM_Onetime(42066, 8536, 8122, state_condition='Stateless', shuffle='True', peak_hour=1, pred_hour=24, LSTM_hour=24, \n",
    "#                 num_unit=num_unit, num_epoch=num_epoch, optimizer=\"Adam\",loss_ftn='mean_squared_error')\n",
    "\n",
    "#Make_LSTM_StepUp(42526, 8605, 8325, state_condition='Stateless', shuffle='False', peak_hour=1, pred_hour=24, LSTM_hour=24, \n",
    "#                 num_unit=num_unit, num_epoch=num_epoch, optimizer=\"Adam\",loss_ftn='mean_squared_error')\n",
    "#Make_LSTM_Onetime(42066, 8536, 8122, state_condition='Stateless', shuffle='False', peak_hour=1, pred_hour=24, LSTM_hour=24, \n",
    "#                 num_unit=num_unit, num_epoch=num_epoch, optimizer=\"Adam\",loss_ftn='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onetime Output!\n",
      "allDataSet:  (59104, 65)\n",
      "allDataSet_size:  59104\n",
      "allDataSet Type:  <class 'pandas.core.frame.DataFrame'>\n",
      "allDataSet_size Type:  <class 'int'>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 40)                5040      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                984       \n",
      "=================================================================\n",
      "Total params: 6,024\n",
      "Trainable params: 6,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "`pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\venv\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1860\u001b[0m                 \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m                 stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\u001b[0m\u001b[0;32m   1862\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\venv\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds)\u001b[0m\n\u001b[0;32m    675\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    677\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\venv\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m    956\u001b[0m                                          \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m    958\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 지정된 파일을 찾을 수 없습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras-2.2.0-py3.5.egg\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\venv\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1866\u001b[0m                     prog=prog)\n\u001b[1;32m-> 1867\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] \"dot.exe\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-281de192349d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m Make_LSTM_Onetime(42317, 8573, 8214, state_condition='Stateless', shuffle='False', peak_hour=1, pred_hour=24, LSTM_hour=36, \n\u001b[1;32m---> 15\u001b[1;33m                  num_unit=num_unit, num_epoch=num_epoch, optimizer=\"Adam\",loss_ftn='mean_squared_error')\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-f0b3efee2cf7>\u001b[0m in \u001b[0;36mMake_LSTM_Onetime\u001b[1;34m(train_index, val_index, test_index, state_condition, shuffle, peak_hour, pred_hour, LSTM_hour, num_unit, num_epoch, optimizer, loss_ftn)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m     \u001b[0mSVG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dot'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'svg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras-2.2.0-py3.5.egg\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras-2.2.0-py3.5.egg\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         raise OSError(\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[1;34m'`pydot` failed to call GraphViz.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[1;34m'Please install GraphViz (https://www.graphviz.org/) '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             'and ensure that its executables are in the $PATH.')\n",
      "\u001b[1;31mOSError\u001b[0m: `pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH."
     ]
    }
   ],
   "source": [
    "num_epoch = 1200\n",
    "num_unit = 40\n",
    "\n",
    "#Make_LSTM_StepUp(42789, 8642, 8421, state_condition='Stateless', shuffle='True', peak_hour=1, pred_hour=24, LSTM_hour=36, \n",
    "#                 num_unit=num_unit, num_epoch=num_epoch, optimizer=\"Adam\",loss_ftn='mean_squared_error')\n",
    "#Make_LSTM_Onetime(42317, 8573, 8214, state_condition='Stateless', shuffle='True', peak_hour=1, pred_hour=24, LSTM_hour=36, \n",
    "#                 num_unit=num_unit, num_epoch=num|_epoch, optimizer=\"Adam\",loss_ftn='mean_squared_error')\n",
    "\n",
    "#Make_LSTM_StepUp(42789, 8642, 8421, state_condition='Stateless', shuffle='False', peak_hour=1, pred_hour=24, LSTM_hour=36, \n",
    "#                 num_unit=num_unit, num_epoch=num_epoch, optimizer=\"Adam\",loss_ftn='mean_squared_error')\n",
    "#Make_LSTM_Onetime(42317, 8573, 8214, state_condition='Stateless', shuffle='False', peak_hour=1, pred_hour=24, LSTM_hour=36, \n",
    "#                 num_unit=num_unit, num_epoch=num_epoch, optimizer=\"Adam\",loss_ftn='mean_squared_error')\n",
    "\n",
    "Make_LSTM_Onetime(42317, 8573, 8214, state_condition='Stateless', shuffle='False', peak_hour=1, pred_hour=24, LSTM_hour=36, \n",
    "                 num_unit=num_unit, num_epoch=num_epoch, optimizer=\"Adam\",loss_ftn='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 1200\n",
    "num_unit =30\n",
    "\n",
    "Make_LSTM_Onetime(42317, 8573, 8214, state_condition='Stateless', shuffle='False', peak_hour=1, pred_hour=24, LSTM_hour=36, \n",
    "                 num_unit=num_unit, num_epoch=num_epoch, optimizer=\"Adam\",loss_ftn='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 1200\n",
    "num_unit =50\n",
    "\n",
    "Make_LSTM_Onetime(42317, 8573, 8214, state_condition='Stateless', shuffle='False', peak_hour=1, pred_hour=24, LSTM_hour=36, \n",
    "                 num_unit=num_unit, num_epoch=num_epoch, optimizer=\"Adam\",loss_ftn='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
