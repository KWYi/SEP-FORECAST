{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from pandas import read_csv\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.models import model_from_json\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "np.random.seed(123)  # for reproducibility\n",
    "\n",
    "# Extra part for memory management\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "#tf.Session(config = config)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_LSTM(state_condition, structure, peak_hour, pred_hour, LSTM_hour, \n",
    "                num_unit, num_epoch, train_index, val_index, test_index, optmzr, loss_ftn):\n",
    "    #file_condition = '1hPEAK_6hLSTM_log'\n",
    "    \n",
    "    len_PRED = pred_hour/peak_hour\n",
    "    len_LSTM = LSTM_hour/peak_hour\n",
    "    file_condition = 'SineTest_'+str(peak_hour)+'hPEAK_'+str(pred_hour)+'hPrediction_'+str(LSTM_hour)+'hLSTM'\n",
    "    \n",
    "    if len_PRED <= 0:\n",
    "        print(\"You entered wrong predction hour or peak hour\")\n",
    "        sys.eixt()\n",
    "    if len_LSTM <= 0:\n",
    "        print(\"You entered wrong LSTM hour or peak hour\")\n",
    "        sys.eixt()\n",
    "        \n",
    "    len_PRED = int(len_PRED)\n",
    "    len_LSTM = int(len_LSTM)\n",
    "       \n",
    "    allDataSet = read_csv('C:\\\\Users\\\\YKW\\\\Desktop\\\\SEP_TEMP\\\\'+file_condition+'.csv', sep=',')\n",
    "    allDataSet_size = len(allDataSet)\n",
    "    \n",
    "    print(\"allDataSet: \", allDataSet.shape)\n",
    "    print(\"allDataSet_size: \", allDataSet_size)\n",
    "    print(\"allDataSet Type: \", type(allDataSet))\n",
    "    print(\"allDataSet_size Type: \", type(allDataSet_size))\n",
    "    \n",
    "    data_count = np.arange(len_LSTM)+1\n",
    "    data_count = data_count* (-1)\n",
    "    data_count = data_count +len_LSTM+1\n",
    "    data_title = []\n",
    "    for bi in range(len(data_count)):\n",
    "        data_title.append('b'+str(int(data_count[bi])))\n",
    "        \n",
    "    pred_count = np.arange(len_PRED)\n",
    "    pred_title = []\n",
    "    for ai in range(len(pred_count)):\n",
    "        pred_title.append('a'+str(int(pred_count[ai])))\n",
    "\n",
    "    #flux = allDataSet[['Year','Mon','Day','Hour','Min','b1','b2','b3','b4','b5','b6']].values\n",
    "    flux = allDataSet[data_title].values\n",
    "    pred_flux = allDataSet[pred_title].values\n",
    "    now_flux = allDataSet['a0'].values\n",
    "     \n",
    "    d_flux = np.array([flux])\n",
    "    d_pred_flux = np.array([pred_flux])\n",
    "    d_now_flux = np.array([now_flux])\n",
    "    \n",
    "    d_flux = np.array(d_flux).reshape(allDataSet_size, len_LSTM,1) ### d_flux = np.array(d_flux).reshape(allDataSet_size, 1, len_LSTM)\n",
    "#    d_pred_flux = np.array(d_pred_flux).reshape(allDataSet_size, 1, len_PRED)\n",
    "    d_pred_flux = np.array(d_pred_flux).reshape(allDataSet_size, len_PRED)\n",
    "    d_now_flux = np.array([d_now_flux]).reshape(allDataSet_size, 1)\n",
    "    \n",
    "\n",
    "    def M_LSTM_Stateful_Inspace(shapes):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(num_unit,batch_input_shape=shapes, stateful=True))\n",
    "        model.add(Dense(len_PRED))\n",
    "        model.summary()\n",
    "        return model\n",
    "            \n",
    "    def M_LSTM_Stateless_Inspace(shapes):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(num_unit,input_shape=shapes, return_sequences=True, stateful=False))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(LSTM(num_unit,input_shape=shapes, return_sequences=True, stateful=False))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(LSTM(num_unit,input_shape=shapes, stateful=False))\n",
    "        model.add(Dense(len_PRED))\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def M_LSTM_Stateful_SeaT(shapes):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(num_unit,batch_input_shape=shapes, stateful=True))\n",
    "        model.add(Dense(7))\n",
    "        model.add(Dense(len_PRED))\n",
    "        model.summary()\n",
    "        return model\n",
    "            \n",
    "    def M_LSTM_Stateless_SeaT(shapes):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(num_unit,input_shape=shapes, stateful=False))\n",
    "        model.add(Dense(7))\n",
    "        model.add(Dense(len_PRED))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "\n",
    "    class PlotLosses(keras.callbacks.Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.i = 0\n",
    "            self.x = []\n",
    "            self.losses = []\n",
    "            self.val_losses = []\n",
    "\n",
    "            self.fig = plt.figure()\n",
    "\n",
    "            self.logs = []\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "            self.logs.append(logs)\n",
    "            self.x.append(self.i)\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.val_losses.append(logs.get('val_loss'))\n",
    "            self.i += 1\n",
    "            clear_output(wait=True)\n",
    "            plt.plot(self.x, self.losses, label=\"loss\")\n",
    "            plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.show();\n",
    "            \n",
    "\n",
    "    if structure is 'Inspace':\n",
    "        weightfileName = 'C:\\\\Users\\\\YKW\\\\Desktop\\\\SEP_TEMP\\\\ReturnSQ_'+state_condition+'_Shuffled_'+'LSTM_GOES15_pgt10_'+file_condition+'_ep'+str(num_epoch)+'_'+str(num_unit)+'units_'+optmzr+'_' + loss_ftn + '_MODEL_WEIGHT'\n",
    "        testfileName = 'C:\\\\Users\\\\YKW\\\\Desktop\\\\SEP_TEMP\\\\ReturnSQ_'+state_condition+'_Shuffled_'+'TEST_'+file_condition+'_ep'+str(num_epoch)+'_'+str(num_unit)+'units_'+optmzr + '_' + loss_ftn + '_Proton Flux Prediction By LSTM Model Train.csv'\n",
    "    else:\n",
    "        if structure is 'SeaT':\n",
    "            weightfileName = 'C:\\\\Users\\\\YKW\\\\Desktop\\\\SEP_TEMP\\\\ReturnSQ_'+state_condition+'_Shuffled_'+'Dense7_LSTM_GOES15_pgt10_'+file_condition+'_ep'+str(num_epoch)+'_'+str(num_unit)+'units_'+optmzr+'_' + loss_ftn + '_MODEL_WEIGHT'\n",
    "            testfileName = 'C:\\\\Users\\\\YKW\\\\Desktop\\\\SEP_TEMP\\\\ReturnSQ_'+state_condition+'_Shuffled_'+'Dense7_TEST_'+file_condition+'_ep'+str(num_epoch)+'_'+str(num_unit)+'units_'+optmzr + '_' + loss_ftn + '_Proton Flux Prediction By LSTM Model Train.csv'\n",
    "        else:\n",
    "            print(\"You entered wrong \")\n",
    "            sys.exit()\n",
    "            \n",
    "\n",
    "    plot_losses = PlotLosses()\n",
    "    \n",
    "    if state_condition is 'Stateful':\n",
    "        inputFluxShape = (1,1,len_LSTM)\n",
    "        if structure is 'Inspace':\n",
    "            model = M_LSTM_Stateful_Inspace(inputFluxShape)\n",
    "        if structure is 'SeaT':\n",
    "            model = M_LSTM_Stateful_SeaT(inputFluxShape)\n",
    "    else:\n",
    "        if state_condition is 'Stateless':\n",
    "            inputFluxShape = (len_LSTM,1) ### inputFluxShape = (1,len_LSTM)\n",
    "            if structure is 'Inspace':\n",
    "                model = M_LSTM_Stateless_Inspace(inputFluxShape)\n",
    "            if structure is 'SeaT':\n",
    "                model = M_LSTM_Stateless_SeaT(inputFluxShape)\n",
    "        else:\n",
    "            print(\"You enterd wrong state_condition!\")\n",
    "            sys.exit()\n",
    " \n",
    "\n",
    "    val_index  = train_index + val_index\n",
    "    test_index = val_index + test_index\n",
    "    \n",
    "    trainX = d_flux[:train_index]\n",
    "    trainY = d_pred_flux[:train_index]\n",
    "        \n",
    "    valX = d_flux[train_index:val_index]\n",
    "    valY = d_pred_flux[train_index:val_index]\n",
    "        \n",
    "    testX = d_flux[val_index:test_index]\n",
    "    testY = d_pred_flux[val_index:test_index]\n",
    "    testNowY = d_now_flux[val_index:test_index]\n",
    "\n",
    "    \n",
    "    model.compile(loss=loss_ftn, optimizer=optmzr, metrics=['acc'])\n",
    "     \n",
    "    if state_condition is 'Stateful':    \n",
    "        for epoch_idx in range(num_epoch):\n",
    "            model.fit(trainX, trainY, epochs=1, batch_size=1, verbose=2, shuffle=False, validation_data=(valX, valY))\n",
    "            model.reset_states()\n",
    "            print(epoch_idx)\n",
    "    else:\n",
    "        if state_condition is 'Stateless': \n",
    "            model.fit(trainX, trainY, epochs=num_epoch, batch_size=2520, verbose=2, validation_data=(valX, valY))\n",
    "\n",
    "        \n",
    "    model_json = model.to_json()\n",
    "    with open(weightfileName + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(weightfileName + \".h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "    json_file = open(weightfileName + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    loaded_model.load_weights(weightfileName + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    loaded_model.compile(loss=loss_ftn, optimizer=optmzr, metrics=['acc'])\n",
    "    \n",
    "\n",
    "    #date_testY = allDataSet['date'].values[val_index:test_index]\n",
    "    f = open(testfileName, 'w')\n",
    "    f.write('Ground')\n",
    "    for i in range(len_PRED):\n",
    "        f.write(','+ 'a'+str(i))\n",
    "    f.write('\\n')\n",
    "        \n",
    "    for idx2 in range(0, len(testX)):\n",
    "\n",
    "        labelY = testY[idx2:idx2+1]\n",
    "        NowY = testNowY[idx2:idx2+1]\n",
    "        \n",
    "        pred = []\n",
    "        pred_in = testX[idx2:idx2+1]\n",
    "        #pred_in = np.array(pred_in)\n",
    "\n",
    "        #resultY = loaded_model.predict(pred_in)\n",
    "        resultY = model.predict(pred_in)\n",
    "         \n",
    "        f.write(str(NowY[0][0]))\n",
    "        for nwr in range(len_PRED):\n",
    "            f.write(',' + str(resultY[0][nwr]))   \n",
    "        f.write('\\n')\n",
    "        pred_in = []\n",
    "        resultY = []\n",
    "        print(idx2)\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allDataSet:  (3600, 72)\n",
      "allDataSet_size:  3600\n",
      "allDataSet Type:  <class 'pandas.core.frame.DataFrame'>\n",
      "allDataSet_size Type:  <class 'int'>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 48, 240)           232320    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48, 240)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 48, 240)           461760    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 48, 240)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 240)               461760    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                5784      \n",
      "=================================================================\n",
      "Total params: 1,161,624\n",
      "Trainable params: 1,161,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2520 samples, validate on 360 samples\n",
      "Epoch 1/600\n",
      " - 3s - loss: 0.5016 - acc: 0.0091 - val_loss: 0.4727 - val_acc: 0.0056\n",
      "Epoch 2/600\n",
      " - 1s - loss: 0.4728 - acc: 0.0155 - val_loss: 0.4335 - val_acc: 0.0056\n",
      "Epoch 3/600\n",
      " - 1s - loss: 0.4337 - acc: 0.0087 - val_loss: 0.3738 - val_acc: 0.0056\n",
      "Epoch 4/600\n",
      " - 1s - loss: 0.3741 - acc: 0.0325 - val_loss: 0.3430 - val_acc: 0.3944\n",
      "Epoch 5/600\n",
      " - 1s - loss: 0.3432 - acc: 0.3738 - val_loss: 0.3021 - val_acc: 0.3833\n",
      "Epoch 6/600\n",
      " - 1s - loss: 0.3024 - acc: 0.3817 - val_loss: 0.2567 - val_acc: 0.5333\n",
      "Epoch 7/600\n",
      " - 1s - loss: 0.2573 - acc: 0.4877 - val_loss: 0.2348 - val_acc: 0.2667\n",
      "Epoch 8/600\n",
      " - 1s - loss: 0.2356 - acc: 0.2869 - val_loss: 0.2001 - val_acc: 0.2444\n",
      "Epoch 9/600\n",
      " - 1s - loss: 0.2008 - acc: 0.2349 - val_loss: 0.1543 - val_acc: 0.2500\n",
      "Epoch 10/600\n",
      " - 1s - loss: 0.1553 - acc: 0.2488 - val_loss: 0.1137 - val_acc: 0.2222\n",
      "Epoch 11/600\n",
      " - 1s - loss: 0.1146 - acc: 0.2250 - val_loss: 0.0614 - val_acc: 0.1278\n",
      "Epoch 12/600\n",
      " - 1s - loss: 0.0628 - acc: 0.1147 - val_loss: 0.0347 - val_acc: 0.0056\n",
      "Epoch 13/600\n",
      " - 1s - loss: 0.0358 - acc: 0.0107 - val_loss: 0.1686 - val_acc: 0.0111\n",
      "Epoch 14/600\n",
      " - 1s - loss: 0.1669 - acc: 0.0536 - val_loss: 0.1285 - val_acc: 0.0056\n",
      "Epoch 15/600\n",
      " - 1s - loss: 0.1265 - acc: 0.0274 - val_loss: 0.0660 - val_acc: 0.0167\n",
      "Epoch 16/600\n",
      " - 1s - loss: 0.0666 - acc: 0.0885 - val_loss: 0.0302 - val_acc: 0.2333\n",
      "Epoch 17/600\n",
      " - 1s - loss: 0.0317 - acc: 0.2262 - val_loss: 0.0770 - val_acc: 0.2111\n",
      "Epoch 18/600\n",
      " - 1s - loss: 0.0782 - acc: 0.2139 - val_loss: 0.0728 - val_acc: 0.3278\n",
      "Epoch 19/600\n",
      " - 1s - loss: 0.0735 - acc: 0.3183 - val_loss: 0.0488 - val_acc: 0.5444\n",
      "Epoch 20/600\n",
      " - 1s - loss: 0.0500 - acc: 0.5024 - val_loss: 0.0469 - val_acc: 0.4667\n",
      "Epoch 21/600\n",
      " - 1s - loss: 0.0482 - acc: 0.4639 - val_loss: 0.0551 - val_acc: 0.4444\n",
      "Epoch 22/600\n",
      " - 1s - loss: 0.0560 - acc: 0.4476 - val_loss: 0.0497 - val_acc: 0.4556\n",
      "Epoch 23/600\n",
      " - 1s - loss: 0.0507 - acc: 0.4587 - val_loss: 0.0343 - val_acc: 0.4889\n",
      "Epoch 24/600\n",
      " - 1s - loss: 0.0352 - acc: 0.4861 - val_loss: 0.0319 - val_acc: 0.5500\n",
      "Epoch 25/600\n",
      " - 1s - loss: 0.0331 - acc: 0.5516 - val_loss: 0.0301 - val_acc: 0.6833\n",
      "Epoch 26/600\n",
      " - 1s - loss: 0.0311 - acc: 0.6433 - val_loss: 0.0195 - val_acc: 0.7278\n",
      "Epoch 27/600\n",
      " - 1s - loss: 0.0205 - acc: 0.6171 - val_loss: 0.0264 - val_acc: 0.5444\n",
      "Epoch 28/600\n",
      " - 1s - loss: 0.0269 - acc: 0.5409 - val_loss: 0.0215 - val_acc: 0.5333\n",
      "Epoch 29/600\n",
      " - 1s - loss: 0.0223 - acc: 0.5290 - val_loss: 0.0125 - val_acc: 0.4611\n",
      "Epoch 30/600\n",
      " - 1s - loss: 0.0135 - acc: 0.4683 - val_loss: 0.0153 - val_acc: 0.4111\n",
      "Epoch 31/600\n",
      " - 1s - loss: 0.0163 - acc: 0.4262 - val_loss: 0.0129 - val_acc: 0.4167\n",
      "Epoch 32/600\n",
      " - 1s - loss: 0.0138 - acc: 0.4036 - val_loss: 0.0116 - val_acc: 0.3833\n",
      "Epoch 33/600\n",
      " - 1s - loss: 0.0126 - acc: 0.3873 - val_loss: 0.0125 - val_acc: 0.3611\n",
      "Epoch 34/600\n",
      " - 1s - loss: 0.0135 - acc: 0.3754 - val_loss: 0.0089 - val_acc: 0.3444\n",
      "Epoch 35/600\n",
      " - 1s - loss: 0.0100 - acc: 0.3452 - val_loss: 0.0081 - val_acc: 0.3333\n",
      "Epoch 36/600\n",
      " - 1s - loss: 0.0091 - acc: 0.3401 - val_loss: 0.0109 - val_acc: 0.4389\n",
      "Epoch 37/600\n",
      " - 1s - loss: 0.0118 - acc: 0.3540 - val_loss: 0.0087 - val_acc: 0.4222\n",
      "Epoch 38/600\n",
      " - 1s - loss: 0.0098 - acc: 0.3889 - val_loss: 0.0058 - val_acc: 0.3667\n",
      "Epoch 39/600\n",
      " - 1s - loss: 0.0067 - acc: 0.3909 - val_loss: 0.0071 - val_acc: 0.3722\n",
      "Epoch 40/600\n",
      " - 1s - loss: 0.0079 - acc: 0.3897 - val_loss: 0.0069 - val_acc: 0.4056\n",
      "Epoch 41/600\n",
      " - 1s - loss: 0.0078 - acc: 0.4131 - val_loss: 0.0046 - val_acc: 0.4389\n",
      "Epoch 42/600\n",
      " - 1s - loss: 0.0056 - acc: 0.4310 - val_loss: 0.0044 - val_acc: 0.4556\n",
      "Epoch 43/600\n",
      " - 1s - loss: 0.0054 - acc: 0.4548 - val_loss: 0.0037 - val_acc: 0.4556\n",
      "Epoch 44/600\n",
      " - 1s - loss: 0.0047 - acc: 0.4560 - val_loss: 0.0027 - val_acc: 0.4333\n",
      "Epoch 45/600\n",
      " - 1s - loss: 0.0036 - acc: 0.4306 - val_loss: 0.0038 - val_acc: 0.4000\n",
      "Epoch 46/600\n",
      " - 1s - loss: 0.0047 - acc: 0.3944 - val_loss: 0.0032 - val_acc: 0.3611\n",
      "Epoch 47/600\n",
      " - 1s - loss: 0.0041 - acc: 0.3730 - val_loss: 0.0019 - val_acc: 0.3889\n",
      "Epoch 48/600\n",
      " - 1s - loss: 0.0028 - acc: 0.3861 - val_loss: 0.0028 - val_acc: 0.3444\n",
      "Epoch 49/600\n",
      " - 1s - loss: 0.0037 - acc: 0.3512 - val_loss: 0.0022 - val_acc: 0.2889\n",
      "Epoch 50/600\n",
      " - 1s - loss: 0.0031 - acc: 0.2889 - val_loss: 0.0018 - val_acc: 0.1889\n",
      "Epoch 51/600\n",
      " - 1s - loss: 0.0026 - acc: 0.2389 - val_loss: 0.0021 - val_acc: 0.1833\n",
      "Epoch 52/600\n",
      " - 1s - loss: 0.0029 - acc: 0.2171 - val_loss: 0.0012 - val_acc: 0.2333\n",
      "Epoch 53/600\n",
      " - 1s - loss: 0.0021 - acc: 0.2754 - val_loss: 0.0016 - val_acc: 0.3111\n",
      "Epoch 54/600\n",
      " - 1s - loss: 0.0025 - acc: 0.3353 - val_loss: 0.0017 - val_acc: 0.4278\n",
      "Epoch 55/600\n",
      " - 1s - loss: 0.0025 - acc: 0.4246 - val_loss: 9.5754e-04 - val_acc: 0.6111\n",
      "Epoch 56/600\n",
      " - 1s - loss: 0.0018 - acc: 0.5230 - val_loss: 0.0012 - val_acc: 0.6167\n",
      "Epoch 57/600\n",
      " - 1s - loss: 0.0020 - acc: 0.5857 - val_loss: 9.5896e-04 - val_acc: 0.6389\n",
      "Epoch 58/600\n",
      " - 1s - loss: 0.0018 - acc: 0.6234 - val_loss: 6.8407e-04 - val_acc: 0.6444\n",
      "Epoch 59/600\n",
      " - 1s - loss: 0.0015 - acc: 0.6750 - val_loss: 8.8993e-04 - val_acc: 0.8278\n",
      "Epoch 60/600\n",
      " - 1s - loss: 0.0017 - acc: 0.7488 - val_loss: 4.8743e-04 - val_acc: 0.8722\n",
      "Epoch 61/600\n",
      " - 1s - loss: 0.0013 - acc: 0.8095 - val_loss: 7.0076e-04 - val_acc: 0.8500\n",
      "Epoch 62/600\n",
      " - 1s - loss: 0.0014 - acc: 0.8151 - val_loss: 7.5891e-04 - val_acc: 0.8222\n",
      "Epoch 63/600\n",
      " - 1s - loss: 0.0016 - acc: 0.7956 - val_loss: 4.2248e-04 - val_acc: 0.8056\n",
      "Epoch 64/600\n",
      " - 1s - loss: 0.0012 - acc: 0.7861 - val_loss: 6.1742e-04 - val_acc: 0.8056\n",
      "Epoch 65/600\n",
      " - 1s - loss: 0.0014 - acc: 0.7639 - val_loss: 4.2456e-04 - val_acc: 0.8000\n",
      "Epoch 66/600\n",
      " - 1s - loss: 0.0012 - acc: 0.7667 - val_loss: 4.3402e-04 - val_acc: 0.7833\n",
      "Epoch 67/600\n",
      " - 1s - loss: 0.0012 - acc: 0.7563 - val_loss: 4.1048e-04 - val_acc: 0.7778\n",
      "Epoch 68/600\n",
      " - 1s - loss: 0.0012 - acc: 0.7444 - val_loss: 2.6991e-04 - val_acc: 0.7833\n",
      "Epoch 69/600\n",
      " - 1s - loss: 0.0011 - acc: 0.7302 - val_loss: 4.2320e-04 - val_acc: 0.7944\n",
      "Epoch 70/600\n",
      " - 1s - loss: 0.0012 - acc: 0.7341 - val_loss: 2.1880e-04 - val_acc: 0.8278\n",
      "Epoch 71/600\n",
      " - 1s - loss: 9.8236e-04 - acc: 0.7321 - val_loss: 3.2354e-04 - val_acc: 0.8667\n",
      "Epoch 72/600\n",
      " - 1s - loss: 0.0011 - acc: 0.7290 - val_loss: 3.0053e-04 - val_acc: 0.8944\n",
      "Epoch 73/600\n",
      " - 1s - loss: 0.0011 - acc: 0.7369 - val_loss: 1.8548e-04 - val_acc: 0.8833\n",
      "Epoch 74/600\n",
      " - 1s - loss: 0.0010 - acc: 0.6972 - val_loss: 2.3141e-04 - val_acc: 0.8722\n",
      "Epoch 75/600\n",
      " - 1s - loss: 0.0011 - acc: 0.6611 - val_loss: 1.4176e-04 - val_acc: 0.8611\n",
      "Epoch 76/600\n",
      " - 1s - loss: 9.3276e-04 - acc: 0.6710 - val_loss: 2.3384e-04 - val_acc: 0.8556\n",
      "Epoch 77/600\n",
      " - 1s - loss: 9.9387e-04 - acc: 0.6655 - val_loss: 1.3872e-04 - val_acc: 0.8556\n",
      "Epoch 78/600\n",
      " - 1s - loss: 8.8907e-04 - acc: 0.6940 - val_loss: 1.5725e-04 - val_acc: 0.8722\n",
      "Epoch 79/600\n",
      " - 1s - loss: 8.9597e-04 - acc: 0.7242 - val_loss: 1.3543e-04 - val_acc: 0.8722\n",
      "Epoch 80/600\n",
      " - 1s - loss: 9.0700e-04 - acc: 0.7512 - val_loss: 1.3032e-04 - val_acc: 0.8833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/600\n",
      " - 1s - loss: 8.7985e-04 - acc: 0.7782 - val_loss: 1.4268e-04 - val_acc: 0.8833\n",
      "Epoch 82/600\n",
      " - 1s - loss: 9.0522e-04 - acc: 0.7893 - val_loss: 9.1604e-05 - val_acc: 0.8722\n",
      "Epoch 83/600\n",
      " - 1s - loss: 8.2202e-04 - acc: 0.8048 - val_loss: 1.2309e-04 - val_acc: 0.8722\n",
      "Epoch 84/600\n",
      " - 1s - loss: 8.8575e-04 - acc: 0.8075 - val_loss: 7.5203e-05 - val_acc: 0.8833\n",
      "Epoch 85/600\n",
      " - 1s - loss: 8.5146e-04 - acc: 0.8218 - val_loss: 1.2124e-04 - val_acc: 0.9000\n",
      "Epoch 86/600\n",
      " - 1s - loss: 8.5747e-04 - acc: 0.8317 - val_loss: 9.0563e-05 - val_acc: 0.9000\n",
      "Epoch 87/600\n",
      " - 1s - loss: 8.3870e-04 - acc: 0.8341 - val_loss: 7.6211e-05 - val_acc: 0.8833\n",
      "Epoch 88/600\n",
      " - 1s - loss: 8.4617e-04 - acc: 0.8302 - val_loss: 7.0273e-05 - val_acc: 0.8889\n",
      "Epoch 89/600\n",
      " - 1s - loss: 8.2864e-04 - acc: 0.8369 - val_loss: 7.5521e-05 - val_acc: 0.8889\n",
      "Epoch 90/600\n",
      " - 1s - loss: 8.1956e-04 - acc: 0.8306 - val_loss: 7.4341e-05 - val_acc: 0.8833\n",
      "Epoch 91/600\n",
      " - 1s - loss: 7.8960e-04 - acc: 0.8282 - val_loss: 6.3412e-05 - val_acc: 0.8833\n",
      "Epoch 92/600\n",
      " - 1s - loss: 8.1587e-04 - acc: 0.8222 - val_loss: 5.8683e-05 - val_acc: 0.8889\n",
      "Epoch 93/600\n",
      " - 1s - loss: 7.9033e-04 - acc: 0.8179 - val_loss: 5.9688e-05 - val_acc: 0.8889\n",
      "Epoch 94/600\n",
      " - 1s - loss: 8.0034e-04 - acc: 0.8198 - val_loss: 5.6132e-05 - val_acc: 0.8833\n",
      "Epoch 95/600\n",
      " - 1s - loss: 7.5577e-04 - acc: 0.8095 - val_loss: 4.9497e-05 - val_acc: 0.8833\n",
      "Epoch 96/600\n",
      " - 1s - loss: 7.3562e-04 - acc: 0.8071 - val_loss: 4.5057e-05 - val_acc: 0.8944\n",
      "Epoch 97/600\n",
      " - 1s - loss: 8.1304e-04 - acc: 0.7897 - val_loss: 4.7621e-05 - val_acc: 0.9000\n",
      "Epoch 98/600\n",
      " - 1s - loss: 7.5754e-04 - acc: 0.7952 - val_loss: 4.8113e-05 - val_acc: 0.9167\n",
      "Epoch 99/600\n",
      " - 1s - loss: 7.7677e-04 - acc: 0.7960 - val_loss: 4.4710e-05 - val_acc: 0.9000\n",
      "Epoch 100/600\n",
      " - 1s - loss: 7.5995e-04 - acc: 0.8000 - val_loss: 4.1441e-05 - val_acc: 0.9056\n",
      "Epoch 101/600\n",
      " - 1s - loss: 7.7739e-04 - acc: 0.7992 - val_loss: 4.2683e-05 - val_acc: 0.9111\n",
      "Epoch 102/600\n",
      " - 1s - loss: 7.7531e-04 - acc: 0.8024 - val_loss: 4.8044e-05 - val_acc: 0.9056\n",
      "Epoch 103/600\n",
      " - 1s - loss: 7.5759e-04 - acc: 0.8028 - val_loss: 3.8358e-05 - val_acc: 0.9000\n",
      "Epoch 104/600\n",
      " - 1s - loss: 7.2886e-04 - acc: 0.8107 - val_loss: 4.3142e-05 - val_acc: 0.9000\n",
      "Epoch 105/600\n",
      " - 1s - loss: 7.7634e-04 - acc: 0.8155 - val_loss: 4.4300e-05 - val_acc: 0.9056\n",
      "Epoch 106/600\n",
      " - 1s - loss: 7.2541e-04 - acc: 0.8179 - val_loss: 3.7252e-05 - val_acc: 0.9056\n",
      "Epoch 107/600\n",
      " - 1s - loss: 7.2121e-04 - acc: 0.8321 - val_loss: 3.5766e-05 - val_acc: 0.8944\n",
      "Epoch 108/600\n",
      " - 1s - loss: 7.2454e-04 - acc: 0.8258 - val_loss: 3.9073e-05 - val_acc: 0.8944\n",
      "Epoch 109/600\n",
      " - 1s - loss: 7.5689e-04 - acc: 0.8131 - val_loss: 3.2537e-05 - val_acc: 0.8944\n",
      "Epoch 110/600\n",
      " - 1s - loss: 7.0810e-04 - acc: 0.8310 - val_loss: 4.5755e-05 - val_acc: 0.9000\n",
      "Epoch 111/600\n",
      " - 1s - loss: 7.4404e-04 - acc: 0.8290 - val_loss: 3.6765e-05 - val_acc: 0.8944\n",
      "Epoch 112/600\n",
      " - 1s - loss: 7.5687e-04 - acc: 0.8226 - val_loss: 3.1122e-05 - val_acc: 0.8944\n",
      "Epoch 113/600\n",
      " - 1s - loss: 7.3272e-04 - acc: 0.8159 - val_loss: 2.8895e-05 - val_acc: 0.9000\n",
      "Epoch 114/600\n",
      " - 1s - loss: 7.1860e-04 - acc: 0.8321 - val_loss: 3.1326e-05 - val_acc: 0.9000\n",
      "Epoch 115/600\n",
      " - 1s - loss: 7.4368e-04 - acc: 0.8278 - val_loss: 2.7539e-05 - val_acc: 0.8889\n",
      "Epoch 116/600\n",
      " - 1s - loss: 7.2105e-04 - acc: 0.8234 - val_loss: 2.9711e-05 - val_acc: 0.8889\n",
      "Epoch 117/600\n",
      " - 1s - loss: 7.5163e-04 - acc: 0.8266 - val_loss: 3.2222e-05 - val_acc: 0.8889\n",
      "Epoch 118/600\n",
      " - 1s - loss: 6.8910e-04 - acc: 0.8302 - val_loss: 3.7095e-05 - val_acc: 0.9000\n",
      "Epoch 119/600\n",
      " - 1s - loss: 7.1886e-04 - acc: 0.8214 - val_loss: 2.8240e-05 - val_acc: 0.9000\n",
      "Epoch 120/600\n",
      " - 1s - loss: 6.8565e-04 - acc: 0.8127 - val_loss: 2.6197e-05 - val_acc: 0.8944\n",
      "Epoch 121/600\n",
      " - 1s - loss: 7.1321e-04 - acc: 0.8115 - val_loss: 2.6025e-05 - val_acc: 0.9111\n",
      "Epoch 122/600\n",
      " - 1s - loss: 7.0240e-04 - acc: 0.8123 - val_loss: 2.3925e-05 - val_acc: 0.9056\n",
      "Epoch 123/600\n",
      " - 1s - loss: 7.0629e-04 - acc: 0.8123 - val_loss: 2.3006e-05 - val_acc: 0.9000\n",
      "Epoch 124/600\n",
      " - 1s - loss: 7.1216e-04 - acc: 0.8115 - val_loss: 2.5211e-05 - val_acc: 0.9056\n",
      "Epoch 125/600\n",
      " - 1s - loss: 7.3058e-04 - acc: 0.8171 - val_loss: 2.7370e-05 - val_acc: 0.9056\n",
      "Epoch 126/600\n",
      " - 1s - loss: 6.8012e-04 - acc: 0.8242 - val_loss: 2.5440e-05 - val_acc: 0.9000\n",
      "Epoch 127/600\n",
      " - 1s - loss: 7.0647e-04 - acc: 0.8139 - val_loss: 2.3152e-05 - val_acc: 0.9000\n",
      "Epoch 128/600\n",
      " - 1s - loss: 6.9871e-04 - acc: 0.8167 - val_loss: 2.6443e-05 - val_acc: 0.9000\n",
      "Epoch 129/600\n",
      " - 1s - loss: 6.7911e-04 - acc: 0.8143 - val_loss: 2.4859e-05 - val_acc: 0.9056\n",
      "Epoch 130/600\n",
      " - 1s - loss: 6.9658e-04 - acc: 0.8298 - val_loss: 2.1585e-05 - val_acc: 0.8944\n",
      "Epoch 131/600\n",
      " - 1s - loss: 6.7653e-04 - acc: 0.8175 - val_loss: 2.3967e-05 - val_acc: 0.8889\n",
      "Epoch 132/600\n",
      " - 1s - loss: 6.9437e-04 - acc: 0.8270 - val_loss: 2.5704e-05 - val_acc: 0.9000\n",
      "Epoch 133/600\n",
      " - 1s - loss: 6.6936e-04 - acc: 0.8302 - val_loss: 2.1430e-05 - val_acc: 0.8944\n",
      "Epoch 134/600\n",
      " - 1s - loss: 6.5962e-04 - acc: 0.8238 - val_loss: 2.3718e-05 - val_acc: 0.8944\n",
      "Epoch 135/600\n",
      " - 1s - loss: 7.1191e-04 - acc: 0.8254 - val_loss: 1.9626e-05 - val_acc: 0.9056\n",
      "Epoch 136/600\n",
      " - 1s - loss: 6.6181e-04 - acc: 0.8373 - val_loss: 2.6578e-05 - val_acc: 0.9056\n",
      "Epoch 137/600\n",
      " - 1s - loss: 6.8387e-04 - acc: 0.8194 - val_loss: 2.0829e-05 - val_acc: 0.9056\n",
      "Epoch 138/600\n",
      " - 1s - loss: 6.8525e-04 - acc: 0.8230 - val_loss: 1.9106e-05 - val_acc: 0.9056\n",
      "Epoch 139/600\n",
      " - 1s - loss: 6.5093e-04 - acc: 0.8123 - val_loss: 1.8784e-05 - val_acc: 0.9000\n",
      "Epoch 140/600\n",
      " - 1s - loss: 6.8125e-04 - acc: 0.8151 - val_loss: 2.1125e-05 - val_acc: 0.9000\n",
      "Epoch 141/600\n",
      " - 1s - loss: 6.8303e-04 - acc: 0.8210 - val_loss: 1.8840e-05 - val_acc: 0.8944\n",
      "Epoch 142/600\n",
      " - 1s - loss: 6.6973e-04 - acc: 0.8381 - val_loss: 1.8379e-05 - val_acc: 0.8944\n",
      "Epoch 143/600\n",
      " - 1s - loss: 6.7290e-04 - acc: 0.8250 - val_loss: 2.5718e-05 - val_acc: 0.9000\n",
      "Epoch 144/600\n",
      " - 1s - loss: 6.7367e-04 - acc: 0.8175 - val_loss: 2.2314e-05 - val_acc: 0.8944\n",
      "Epoch 145/600\n",
      " - 1s - loss: 6.6679e-04 - acc: 0.8230 - val_loss: 1.7660e-05 - val_acc: 0.9167\n",
      "Epoch 146/600\n",
      " - 1s - loss: 6.5436e-04 - acc: 0.8226 - val_loss: 2.3058e-05 - val_acc: 0.9167\n",
      "Epoch 147/600\n",
      " - 1s - loss: 6.5728e-04 - acc: 0.8187 - val_loss: 1.6646e-05 - val_acc: 0.9056\n",
      "Epoch 148/600\n",
      " - 1s - loss: 6.2922e-04 - acc: 0.8262 - val_loss: 1.9994e-05 - val_acc: 0.9111\n",
      "Epoch 149/600\n",
      " - 1s - loss: 6.4334e-04 - acc: 0.8111 - val_loss: 1.7473e-05 - val_acc: 0.9111\n",
      "Epoch 150/600\n",
      " - 1s - loss: 6.4921e-04 - acc: 0.8202 - val_loss: 1.9999e-05 - val_acc: 0.9167\n",
      "Epoch 151/600\n",
      " - 1s - loss: 6.4159e-04 - acc: 0.8234 - val_loss: 2.0241e-05 - val_acc: 0.9111\n",
      "Epoch 152/600\n",
      " - 1s - loss: 6.2820e-04 - acc: 0.8107 - val_loss: 1.8109e-05 - val_acc: 0.9000\n",
      "Epoch 153/600\n",
      " - 1s - loss: 6.6030e-04 - acc: 0.8167 - val_loss: 2.8244e-05 - val_acc: 0.9056\n",
      "Epoch 154/600\n",
      " - 1s - loss: 6.5073e-04 - acc: 0.8206 - val_loss: 1.8579e-05 - val_acc: 0.9056\n",
      "Epoch 155/600\n",
      " - 1s - loss: 6.4018e-04 - acc: 0.8155 - val_loss: 1.5652e-05 - val_acc: 0.8944\n",
      "Epoch 156/600\n",
      " - 1s - loss: 6.2588e-04 - acc: 0.8254 - val_loss: 1.6720e-05 - val_acc: 0.9000\n",
      "Epoch 157/600\n",
      " - 1s - loss: 6.6018e-04 - acc: 0.8234 - val_loss: 4.2396e-05 - val_acc: 0.9056\n",
      "Epoch 158/600\n",
      " - 1s - loss: 6.5345e-04 - acc: 0.8226 - val_loss: 1.5839e-05 - val_acc: 0.9111\n",
      "Epoch 159/600\n",
      " - 1s - loss: 6.0639e-04 - acc: 0.8270 - val_loss: 2.8885e-05 - val_acc: 0.9056\n",
      "Epoch 160/600\n",
      " - 1s - loss: 6.2291e-04 - acc: 0.8234 - val_loss: 2.9226e-05 - val_acc: 0.9111\n",
      "Epoch 161/600\n",
      " - 1s - loss: 6.4611e-04 - acc: 0.8270 - val_loss: 2.5688e-05 - val_acc: 0.9111\n",
      "Epoch 162/600\n",
      " - 1s - loss: 6.3563e-04 - acc: 0.8290 - val_loss: 4.5191e-05 - val_acc: 0.9000\n",
      "Epoch 163/600\n",
      " - 1s - loss: 6.4662e-04 - acc: 0.8234 - val_loss: 1.7112e-05 - val_acc: 0.9056\n",
      "Epoch 164/600\n",
      " - 1s - loss: 6.1612e-04 - acc: 0.8127 - val_loss: 4.2710e-05 - val_acc: 0.9167\n",
      "Epoch 165/600\n",
      " - 1s - loss: 6.3390e-04 - acc: 0.8234 - val_loss: 1.8079e-05 - val_acc: 0.9056\n",
      "Epoch 166/600\n",
      " - 1s - loss: 6.2882e-04 - acc: 0.8234 - val_loss: 1.3969e-05 - val_acc: 0.9056\n",
      "Epoch 167/600\n",
      " - 1s - loss: 5.9751e-04 - acc: 0.8258 - val_loss: 3.0697e-05 - val_acc: 0.9111\n",
      "Epoch 168/600\n",
      " - 1s - loss: 6.2187e-04 - acc: 0.8286 - val_loss: 1.7816e-05 - val_acc: 0.9056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/600\n",
      " - 1s - loss: 6.1067e-04 - acc: 0.8282 - val_loss: 1.6447e-05 - val_acc: 0.9111\n",
      "Epoch 170/600\n",
      " - 1s - loss: 5.9443e-04 - acc: 0.8294 - val_loss: 4.0518e-05 - val_acc: 0.9167\n",
      "Epoch 171/600\n",
      " - 1s - loss: 6.0878e-04 - acc: 0.8250 - val_loss: 1.3389e-05 - val_acc: 0.9167\n",
      "Epoch 172/600\n",
      " - 1s - loss: 5.9462e-04 - acc: 0.8274 - val_loss: 2.7199e-05 - val_acc: 0.9056\n",
      "Epoch 173/600\n",
      " - 1s - loss: 6.0657e-04 - acc: 0.8226 - val_loss: 2.7371e-05 - val_acc: 0.9111\n",
      "Epoch 174/600\n",
      " - 1s - loss: 6.1662e-04 - acc: 0.8214 - val_loss: 1.8135e-05 - val_acc: 0.9111\n",
      "Epoch 175/600\n",
      " - 1s - loss: 5.5225e-04 - acc: 0.8202 - val_loss: 3.5398e-05 - val_acc: 0.9056\n",
      "Epoch 176/600\n",
      " - 1s - loss: 6.2373e-04 - acc: 0.8218 - val_loss: 1.6742e-05 - val_acc: 0.9222\n",
      "Epoch 177/600\n",
      " - 1s - loss: 6.2231e-04 - acc: 0.8230 - val_loss: 4.9351e-05 - val_acc: 0.9278\n",
      "Epoch 178/600\n",
      " - 1s - loss: 6.2247e-04 - acc: 0.8254 - val_loss: 5.3015e-05 - val_acc: 0.9056\n",
      "Epoch 179/600\n",
      " - 1s - loss: 6.4100e-04 - acc: 0.8147 - val_loss: 1.6891e-05 - val_acc: 0.9222\n",
      "Epoch 180/600\n",
      " - 1s - loss: 6.1424e-04 - acc: 0.8321 - val_loss: 4.3878e-05 - val_acc: 0.9333\n",
      "Epoch 181/600\n",
      " - 1s - loss: 6.3344e-04 - acc: 0.8258 - val_loss: 4.2049e-05 - val_acc: 0.9111\n",
      "Epoch 182/600\n",
      " - 1s - loss: 6.3672e-04 - acc: 0.8155 - val_loss: 1.6009e-05 - val_acc: 0.9167\n",
      "Epoch 183/600\n",
      " - 1s - loss: 5.9485e-04 - acc: 0.8306 - val_loss: 3.5288e-05 - val_acc: 0.9167\n",
      "Epoch 184/600\n",
      " - 1s - loss: 5.8918e-04 - acc: 0.8218 - val_loss: 2.4215e-05 - val_acc: 0.9111\n",
      "Epoch 185/600\n",
      " - 1s - loss: 5.8288e-04 - acc: 0.8254 - val_loss: 1.0826e-05 - val_acc: 0.9222\n",
      "Epoch 186/600\n",
      " - 1s - loss: 5.9245e-04 - acc: 0.8234 - val_loss: 3.0085e-05 - val_acc: 0.9278\n",
      "Epoch 187/600\n",
      " - 1s - loss: 5.9654e-04 - acc: 0.8313 - val_loss: 1.4668e-05 - val_acc: 0.9167\n",
      "Epoch 188/600\n",
      " - 1s - loss: 5.8225e-04 - acc: 0.8270 - val_loss: 2.7084e-05 - val_acc: 0.9222\n",
      "Epoch 189/600\n",
      " - 1s - loss: 5.8979e-04 - acc: 0.8286 - val_loss: 1.2707e-05 - val_acc: 0.9111\n",
      "Epoch 190/600\n",
      " - 1s - loss: 6.0140e-04 - acc: 0.8206 - val_loss: 2.6101e-05 - val_acc: 0.9167\n",
      "Epoch 191/600\n",
      " - 1s - loss: 5.7905e-04 - acc: 0.8222 - val_loss: 2.4045e-05 - val_acc: 0.9278\n",
      "Epoch 192/600\n",
      " - 1s - loss: 5.6516e-04 - acc: 0.8313 - val_loss: 1.1730e-05 - val_acc: 0.9056\n",
      "Epoch 193/600\n",
      " - 1s - loss: 5.5937e-04 - acc: 0.8290 - val_loss: 1.8660e-05 - val_acc: 0.9056\n",
      "Epoch 194/600\n",
      " - 1s - loss: 5.8300e-04 - acc: 0.8345 - val_loss: 1.1202e-05 - val_acc: 0.9167\n",
      "Epoch 195/600\n",
      " - 1s - loss: 5.6597e-04 - acc: 0.8325 - val_loss: 2.7844e-05 - val_acc: 0.9278\n",
      "Epoch 196/600\n",
      " - 1s - loss: 5.7367e-04 - acc: 0.8250 - val_loss: 1.4527e-05 - val_acc: 0.9333\n",
      "Epoch 197/600\n",
      " - 1s - loss: 5.8466e-04 - acc: 0.8242 - val_loss: 1.9279e-05 - val_acc: 0.9278\n",
      "Epoch 198/600\n",
      " - 1s - loss: 5.6746e-04 - acc: 0.8214 - val_loss: 2.6053e-05 - val_acc: 0.9167\n",
      "Epoch 199/600\n",
      " - 1s - loss: 5.7895e-04 - acc: 0.8190 - val_loss: 1.2812e-05 - val_acc: 0.9278\n",
      "Epoch 200/600\n",
      " - 1s - loss: 5.4490e-04 - acc: 0.8214 - val_loss: 1.8075e-05 - val_acc: 0.9333\n",
      "Epoch 201/600\n",
      " - 1s - loss: 5.7854e-04 - acc: 0.8349 - val_loss: 1.5263e-05 - val_acc: 0.9333\n",
      "Epoch 202/600\n",
      " - 1s - loss: 5.3017e-04 - acc: 0.8266 - val_loss: 1.8265e-05 - val_acc: 0.9222\n",
      "Epoch 203/600\n",
      " - 1s - loss: 5.5681e-04 - acc: 0.8234 - val_loss: 1.5900e-05 - val_acc: 0.9111\n",
      "Epoch 204/600\n",
      " - 1s - loss: 5.3938e-04 - acc: 0.8302 - val_loss: 3.6658e-05 - val_acc: 0.9167\n",
      "Epoch 205/600\n",
      " - 1s - loss: 5.8081e-04 - acc: 0.8270 - val_loss: 1.4159e-05 - val_acc: 0.9167\n",
      "Epoch 206/600\n",
      " - 1s - loss: 5.5155e-04 - acc: 0.8278 - val_loss: 3.4772e-05 - val_acc: 0.9056\n",
      "Epoch 207/600\n",
      " - 1s - loss: 5.6660e-04 - acc: 0.8234 - val_loss: 2.1217e-05 - val_acc: 0.9222\n",
      "Epoch 208/600\n",
      " - 1s - loss: 5.4341e-04 - acc: 0.8246 - val_loss: 3.9046e-05 - val_acc: 0.9111\n",
      "Epoch 209/600\n",
      " - 1s - loss: 5.7413e-04 - acc: 0.8333 - val_loss: 2.1630e-05 - val_acc: 0.9222\n",
      "Epoch 210/600\n",
      " - 1s - loss: 5.5272e-04 - acc: 0.8357 - val_loss: 1.5735e-05 - val_acc: 0.9056\n",
      "Epoch 211/600\n",
      " - 1s - loss: 5.4953e-04 - acc: 0.8163 - val_loss: 1.9298e-05 - val_acc: 0.9167\n",
      "Epoch 212/600\n",
      " - 1s - loss: 5.5218e-04 - acc: 0.8226 - val_loss: 3.1956e-05 - val_acc: 0.9333\n",
      "Epoch 213/600\n",
      " - 1s - loss: 5.7360e-04 - acc: 0.8286 - val_loss: 1.3455e-05 - val_acc: 0.9278\n",
      "Epoch 214/600\n",
      " - 1s - loss: 5.5879e-04 - acc: 0.8226 - val_loss: 1.7938e-05 - val_acc: 0.9222\n",
      "Epoch 215/600\n",
      " - 1s - loss: 5.5665e-04 - acc: 0.8254 - val_loss: 2.1492e-05 - val_acc: 0.9333\n",
      "Epoch 216/600\n",
      " - 1s - loss: 5.4322e-04 - acc: 0.8294 - val_loss: 5.8154e-05 - val_acc: 0.9222\n",
      "Epoch 217/600\n",
      " - 1s - loss: 5.6815e-04 - acc: 0.8298 - val_loss: 4.6359e-05 - val_acc: 0.9278\n",
      "Epoch 218/600\n",
      " - 1s - loss: 5.5731e-04 - acc: 0.8214 - val_loss: 2.4762e-05 - val_acc: 0.9167\n",
      "Epoch 219/600\n",
      " - 1s - loss: 5.4661e-04 - acc: 0.8210 - val_loss: 4.5712e-05 - val_acc: 0.9111\n",
      "Epoch 220/600\n",
      " - 1s - loss: 5.5203e-04 - acc: 0.8325 - val_loss: 1.7060e-04 - val_acc: 0.9167\n",
      "Epoch 221/600\n",
      " - 1s - loss: 6.3279e-04 - acc: 0.8313 - val_loss: 1.0718e-04 - val_acc: 0.9111\n",
      "Epoch 222/600\n",
      " - 1s - loss: 6.1176e-04 - acc: 0.8194 - val_loss: 2.9127e-05 - val_acc: 0.9278\n",
      "Epoch 223/600\n",
      " - 1s - loss: 5.2877e-04 - acc: 0.8417 - val_loss: 6.5554e-05 - val_acc: 0.9222\n",
      "Epoch 224/600\n",
      " - 1s - loss: 5.6981e-04 - acc: 0.8317 - val_loss: 1.7780e-04 - val_acc: 0.9056\n",
      "Epoch 225/600\n",
      " - 1s - loss: 6.7592e-04 - acc: 0.8183 - val_loss: 1.6607e-04 - val_acc: 0.9222\n",
      "Epoch 226/600\n",
      " - 1s - loss: 6.6568e-04 - acc: 0.8310 - val_loss: 5.7354e-05 - val_acc: 0.9167\n",
      "Epoch 227/600\n",
      " - 1s - loss: 5.7920e-04 - acc: 0.8310 - val_loss: 1.5647e-05 - val_acc: 0.9333\n",
      "Epoch 228/600\n",
      " - 1s - loss: 4.9482e-04 - acc: 0.8325 - val_loss: 4.5308e-05 - val_acc: 0.9333\n",
      "Epoch 229/600\n",
      " - 1s - loss: 5.7075e-04 - acc: 0.8202 - val_loss: 9.5381e-05 - val_acc: 0.9000\n",
      "Epoch 230/600\n",
      " - 1s - loss: 5.9996e-04 - acc: 0.8155 - val_loss: 1.1161e-04 - val_acc: 0.9222\n",
      "Epoch 231/600\n",
      " - 1s - loss: 6.0159e-04 - acc: 0.8278 - val_loss: 2.8498e-05 - val_acc: 0.9167\n",
      "Epoch 232/600\n",
      " - 1s - loss: 5.1211e-04 - acc: 0.8198 - val_loss: 1.6589e-05 - val_acc: 0.9222\n",
      "Epoch 233/600\n",
      " - 1s - loss: 5.1732e-04 - acc: 0.8274 - val_loss: 4.0193e-05 - val_acc: 0.9222\n",
      "Epoch 234/600\n",
      " - 1s - loss: 5.4850e-04 - acc: 0.8413 - val_loss: 8.0577e-05 - val_acc: 0.9056\n",
      "Epoch 235/600\n",
      " - 1s - loss: 5.7738e-04 - acc: 0.8179 - val_loss: 6.0529e-05 - val_acc: 0.9222\n",
      "Epoch 236/600\n",
      " - 1s - loss: 5.4752e-04 - acc: 0.8274 - val_loss: 1.0860e-05 - val_acc: 0.9222\n",
      "Epoch 237/600\n",
      " - 1s - loss: 5.1026e-04 - acc: 0.8278 - val_loss: 2.8854e-05 - val_acc: 0.9222\n",
      "Epoch 238/600\n",
      " - 1s - loss: 5.3494e-04 - acc: 0.8258 - val_loss: 6.4414e-05 - val_acc: 0.9222\n",
      "Epoch 239/600\n",
      " - 1s - loss: 5.5571e-04 - acc: 0.8329 - val_loss: 3.4660e-05 - val_acc: 0.9222\n",
      "Epoch 240/600\n",
      " - 1s - loss: 5.3896e-04 - acc: 0.8337 - val_loss: 2.6237e-05 - val_acc: 0.9222\n",
      "Epoch 241/600\n",
      " - 1s - loss: 5.2279e-04 - acc: 0.8302 - val_loss: 1.1480e-05 - val_acc: 0.9222\n",
      "Epoch 242/600\n",
      " - 1s - loss: 5.0054e-04 - acc: 0.8286 - val_loss: 1.0091e-05 - val_acc: 0.9111\n",
      "Epoch 243/600\n",
      " - 1s - loss: 4.8757e-04 - acc: 0.8377 - val_loss: 7.5490e-06 - val_acc: 0.9167\n",
      "Epoch 244/600\n",
      " - 1s - loss: 4.8205e-04 - acc: 0.8274 - val_loss: 7.3975e-06 - val_acc: 0.9167\n",
      "Epoch 245/600\n",
      " - 1s - loss: 5.0379e-04 - acc: 0.8290 - val_loss: 1.0187e-05 - val_acc: 0.9278\n",
      "Epoch 246/600\n",
      " - 1s - loss: 4.8425e-04 - acc: 0.8313 - val_loss: 8.9511e-06 - val_acc: 0.9222\n",
      "Epoch 247/600\n",
      " - 1s - loss: 4.9678e-04 - acc: 0.8310 - val_loss: 1.3081e-05 - val_acc: 0.9222\n",
      "Epoch 248/600\n",
      " - 1s - loss: 5.0164e-04 - acc: 0.8313 - val_loss: 1.1924e-05 - val_acc: 0.9222\n",
      "Epoch 249/600\n",
      " - 1s - loss: 5.0875e-04 - acc: 0.8294 - val_loss: 2.0504e-05 - val_acc: 0.9222\n",
      "Epoch 250/600\n",
      " - 1s - loss: 4.9400e-04 - acc: 0.8290 - val_loss: 3.3930e-05 - val_acc: 0.9111\n",
      "Epoch 251/600\n",
      " - 1s - loss: 5.2332e-04 - acc: 0.8266 - val_loss: 6.2642e-05 - val_acc: 0.9278\n",
      "Epoch 252/600\n",
      " - 1s - loss: 5.3051e-04 - acc: 0.8317 - val_loss: 1.1256e-05 - val_acc: 0.9333\n",
      "Epoch 253/600\n",
      " - 1s - loss: 4.7856e-04 - acc: 0.8258 - val_loss: 1.0404e-05 - val_acc: 0.9222\n",
      "Epoch 254/600\n",
      " - 1s - loss: 4.6603e-04 - acc: 0.8250 - val_loss: 6.6132e-05 - val_acc: 0.9167\n",
      "Epoch 255/600\n",
      " - 1s - loss: 5.3596e-04 - acc: 0.8246 - val_loss: 1.4903e-04 - val_acc: 0.9111\n",
      "Epoch 256/600\n",
      " - 1s - loss: 6.4055e-04 - acc: 0.8266 - val_loss: 3.4170e-04 - val_acc: 0.9111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257/600\n",
      " - 1s - loss: 7.9881e-04 - acc: 0.8345 - val_loss: 4.3625e-04 - val_acc: 0.9111\n",
      "Epoch 258/600\n",
      " - 1s - loss: 8.7185e-04 - acc: 0.8206 - val_loss: 4.4470e-04 - val_acc: 0.9111\n",
      "Epoch 259/600\n",
      " - 1s - loss: 8.9541e-04 - acc: 0.8226 - val_loss: 3.2784e-04 - val_acc: 0.9111\n",
      "Epoch 260/600\n",
      " - 1s - loss: 7.8696e-04 - acc: 0.8270 - val_loss: 1.7561e-04 - val_acc: 0.9111\n",
      "Epoch 261/600\n",
      " - 1s - loss: 6.5193e-04 - acc: 0.8317 - val_loss: 7.1116e-05 - val_acc: 0.9222\n",
      "Epoch 262/600\n",
      " - 1s - loss: 5.4049e-04 - acc: 0.8226 - val_loss: 3.7148e-05 - val_acc: 0.9222\n",
      "Epoch 263/600\n",
      " - 1s - loss: 4.8794e-04 - acc: 0.8425 - val_loss: 3.3113e-05 - val_acc: 0.9278\n",
      "Epoch 264/600\n",
      " - 1s - loss: 5.0753e-04 - acc: 0.8262 - val_loss: 2.8287e-05 - val_acc: 0.9111\n",
      "Epoch 265/600\n",
      " - 1s - loss: 4.7849e-04 - acc: 0.8218 - val_loss: 3.9983e-05 - val_acc: 0.9222\n",
      "Epoch 266/600\n",
      " - 1s - loss: 4.9036e-04 - acc: 0.8313 - val_loss: 2.7639e-05 - val_acc: 0.9056\n",
      "Epoch 267/600\n",
      " - 1s - loss: 4.9011e-04 - acc: 0.8250 - val_loss: 5.1036e-05 - val_acc: 0.9222\n",
      "Epoch 268/600\n",
      " - 1s - loss: 4.9687e-04 - acc: 0.8270 - val_loss: 4.4880e-05 - val_acc: 0.9056\n",
      "Epoch 269/600\n",
      " - 1s - loss: 5.0755e-04 - acc: 0.8369 - val_loss: 4.9538e-05 - val_acc: 0.9278\n",
      "Epoch 270/600\n",
      " - 1s - loss: 5.0164e-04 - acc: 0.8290 - val_loss: 7.3756e-05 - val_acc: 0.9111\n",
      "Epoch 271/600\n",
      " - 1s - loss: 5.4162e-04 - acc: 0.8313 - val_loss: 9.5387e-05 - val_acc: 0.9278\n",
      "Epoch 272/600\n",
      " - 1s - loss: 5.3563e-04 - acc: 0.8397 - val_loss: 1.1457e-04 - val_acc: 0.9167\n",
      "Epoch 273/600\n",
      " - 1s - loss: 5.7983e-04 - acc: 0.8353 - val_loss: 1.5732e-04 - val_acc: 0.9167\n",
      "Epoch 274/600\n",
      " - 1s - loss: 6.2163e-04 - acc: 0.8389 - val_loss: 2.5043e-04 - val_acc: 0.9000\n",
      "Epoch 275/600\n",
      " - 1s - loss: 7.1745e-04 - acc: 0.8214 - val_loss: 5.4857e-04 - val_acc: 0.9111\n",
      "Epoch 276/600\n",
      " - 1s - loss: 9.6761e-04 - acc: 0.8349 - val_loss: 7.5270e-04 - val_acc: 0.8889\n",
      "Epoch 277/600\n",
      " - 1s - loss: 0.0012 - acc: 0.8214 - val_loss: 0.0011 - val_acc: 0.8944\n",
      "Epoch 278/600\n",
      " - 1s - loss: 0.0015 - acc: 0.8310 - val_loss: 0.0013 - val_acc: 0.8667\n",
      "Epoch 279/600\n",
      " - 1s - loss: 0.0018 - acc: 0.8143 - val_loss: 0.0017 - val_acc: 0.8722\n",
      "Epoch 280/600\n",
      " - 1s - loss: 0.0021 - acc: 0.8353 - val_loss: 0.0013 - val_acc: 0.8833\n",
      "Epoch 281/600\n",
      " - 1s - loss: 0.0017 - acc: 0.8159 - val_loss: 7.2205e-04 - val_acc: 0.8889\n",
      "Epoch 282/600\n",
      " - 1s - loss: 0.0011 - acc: 0.8270 - val_loss: 8.2023e-05 - val_acc: 0.8944\n",
      "Epoch 283/600\n",
      " - 1s - loss: 5.1581e-04 - acc: 0.8210 - val_loss: 1.3075e-04 - val_acc: 0.9056\n",
      "Epoch 284/600\n",
      " - 1s - loss: 5.8729e-04 - acc: 0.8254 - val_loss: 7.6792e-04 - val_acc: 0.9000\n",
      "Epoch 285/600\n",
      " - 1s - loss: 0.0011 - acc: 0.8393 - val_loss: 8.6005e-04 - val_acc: 0.8722\n",
      "Epoch 286/600\n",
      " - 1s - loss: 0.0013 - acc: 0.8143 - val_loss: 6.1805e-04 - val_acc: 0.9222\n",
      "Epoch 287/600\n",
      " - 1s - loss: 0.0010 - acc: 0.8361 - val_loss: 8.5169e-05 - val_acc: 0.9056\n",
      "Epoch 288/600\n",
      " - 1s - loss: 5.3290e-04 - acc: 0.8329 - val_loss: 7.8408e-05 - val_acc: 0.8944\n",
      "Epoch 289/600\n",
      " - 1s - loss: 5.1745e-04 - acc: 0.8230 - val_loss: 3.9932e-04 - val_acc: 0.9000\n",
      "Epoch 290/600\n",
      " - 1s - loss: 8.4700e-04 - acc: 0.8433 - val_loss: 4.8166e-04 - val_acc: 0.8778\n",
      "Epoch 291/600\n",
      " - 1s - loss: 9.2270e-04 - acc: 0.8143 - val_loss: 2.5935e-04 - val_acc: 0.8889\n",
      "Epoch 292/600\n",
      " - 1s - loss: 7.1745e-04 - acc: 0.8254 - val_loss: 3.1930e-05 - val_acc: 0.9056\n",
      "Epoch 293/600\n",
      " - 1s - loss: 4.9590e-04 - acc: 0.8298 - val_loss: 9.8813e-05 - val_acc: 0.9056\n",
      "Epoch 294/600\n",
      " - 1s - loss: 5.5114e-04 - acc: 0.8230 - val_loss: 3.5535e-04 - val_acc: 0.9111\n",
      "Epoch 295/600\n",
      " - 1s - loss: 8.5277e-04 - acc: 0.8357 - val_loss: 4.5606e-04 - val_acc: 0.9000\n",
      "Epoch 296/600\n",
      " - 1s - loss: 9.4142e-04 - acc: 0.8175 - val_loss: 3.1544e-04 - val_acc: 0.9111\n",
      "Epoch 297/600\n",
      " - 1s - loss: 7.6120e-04 - acc: 0.8242 - val_loss: 4.7654e-05 - val_acc: 0.9111\n",
      "Epoch 298/600\n",
      " - 1s - loss: 4.9231e-04 - acc: 0.8349 - val_loss: 4.6129e-05 - val_acc: 0.9167\n",
      "Epoch 299/600\n",
      " - 1s - loss: 4.8007e-04 - acc: 0.8361 - val_loss: 1.9680e-04 - val_acc: 0.9111\n",
      "Epoch 300/600\n",
      " - 1s - loss: 6.5682e-04 - acc: 0.8389 - val_loss: 2.3453e-04 - val_acc: 0.8944\n",
      "Epoch 301/600\n",
      " - 1s - loss: 6.7639e-04 - acc: 0.8270 - val_loss: 9.6285e-05 - val_acc: 0.9167\n",
      "Epoch 302/600\n",
      " - 1s - loss: 5.4503e-04 - acc: 0.8337 - val_loss: 1.0904e-05 - val_acc: 0.9111\n",
      "Epoch 303/600\n",
      " - 1s - loss: 4.2634e-04 - acc: 0.8206 - val_loss: 1.2589e-04 - val_acc: 0.9056\n",
      "Epoch 304/600\n",
      " - 1s - loss: 5.7550e-04 - acc: 0.8313 - val_loss: 2.8215e-04 - val_acc: 0.9167\n",
      "Epoch 305/600\n",
      " - 1s - loss: 6.9666e-04 - acc: 0.8313 - val_loss: 1.7119e-04 - val_acc: 0.9056\n",
      "Epoch 306/600\n",
      " - 1s - loss: 5.8227e-04 - acc: 0.8254 - val_loss: 2.3894e-05 - val_acc: 0.9278\n",
      "Epoch 307/600\n",
      " - 1s - loss: 4.5423e-04 - acc: 0.8313 - val_loss: 8.9181e-05 - val_acc: 0.9278\n",
      "Epoch 308/600\n",
      " - 1s - loss: 5.1664e-04 - acc: 0.8337 - val_loss: 2.5188e-04 - val_acc: 0.8944\n",
      "Epoch 309/600\n",
      " - 1s - loss: 6.9856e-04 - acc: 0.8294 - val_loss: 3.2313e-04 - val_acc: 0.9056\n",
      "Epoch 310/600\n",
      " - 1s - loss: 7.3278e-04 - acc: 0.8365 - val_loss: 1.1865e-04 - val_acc: 0.9000\n",
      "Epoch 311/600\n",
      " - 1s - loss: 5.5646e-04 - acc: 0.8341 - val_loss: 1.0607e-05 - val_acc: 0.9167\n",
      "Epoch 312/600\n",
      " - 1s - loss: 4.4489e-04 - acc: 0.8377 - val_loss: 9.4196e-05 - val_acc: 0.9167\n",
      "Epoch 313/600\n",
      " - 1s - loss: 5.3344e-04 - acc: 0.8270 - val_loss: 2.1898e-04 - val_acc: 0.8944\n",
      "Epoch 314/600\n",
      " - 1s - loss: 6.8235e-04 - acc: 0.8202 - val_loss: 2.8635e-04 - val_acc: 0.9111\n",
      "Epoch 315/600\n",
      " - 1s - loss: 6.8282e-04 - acc: 0.8345 - val_loss: 5.0823e-05 - val_acc: 0.9167\n",
      "Epoch 316/600\n",
      " - 1s - loss: 4.7733e-04 - acc: 0.8282 - val_loss: 5.6128e-05 - val_acc: 0.9111\n",
      "Epoch 317/600\n",
      " - 1s - loss: 4.8649e-04 - acc: 0.8278 - val_loss: 3.0514e-04 - val_acc: 0.9222\n",
      "Epoch 318/600\n",
      " - 1s - loss: 7.0485e-04 - acc: 0.8262 - val_loss: 3.2004e-04 - val_acc: 0.8944\n",
      "Epoch 319/600\n",
      " - 1s - loss: 7.3076e-04 - acc: 0.8294 - val_loss: 1.0708e-04 - val_acc: 0.9333\n",
      "Epoch 320/600\n",
      " - 1s - loss: 5.2942e-04 - acc: 0.8302 - val_loss: 1.8407e-05 - val_acc: 0.9111\n",
      "Epoch 321/600\n",
      " - 1s - loss: 4.3968e-04 - acc: 0.8377 - val_loss: 9.1627e-05 - val_acc: 0.9000\n",
      "Epoch 322/600\n",
      " - 1s - loss: 5.2399e-04 - acc: 0.8254 - val_loss: 2.1691e-04 - val_acc: 0.9222\n",
      "Epoch 323/600\n",
      " - 1s - loss: 6.3781e-04 - acc: 0.8258 - val_loss: 1.8041e-04 - val_acc: 0.9056\n",
      "Epoch 324/600\n",
      " - 1s - loss: 6.0991e-04 - acc: 0.8210 - val_loss: 1.0621e-04 - val_acc: 0.9167\n",
      "Epoch 325/600\n",
      " - 1s - loss: 5.1433e-04 - acc: 0.8437 - val_loss: 2.4001e-05 - val_acc: 0.9056\n",
      "Epoch 326/600\n",
      " - 1s - loss: 4.2838e-04 - acc: 0.8238 - val_loss: 2.3928e-05 - val_acc: 0.9111\n",
      "Epoch 327/600\n",
      " - 1s - loss: 4.4682e-04 - acc: 0.8270 - val_loss: 1.9328e-04 - val_acc: 0.9222\n",
      "Epoch 328/600\n",
      " - 1s - loss: 5.6862e-04 - acc: 0.8329 - val_loss: 2.4018e-04 - val_acc: 0.8944\n",
      "Epoch 329/600\n",
      " - 1s - loss: 6.8350e-04 - acc: 0.8282 - val_loss: 2.5793e-04 - val_acc: 0.9222\n",
      "Epoch 330/600\n",
      " - 1s - loss: 6.4139e-04 - acc: 0.8278 - val_loss: 6.0689e-05 - val_acc: 0.8944\n",
      "Epoch 331/600\n",
      " - 1s - loss: 4.7215e-04 - acc: 0.8325 - val_loss: 1.1050e-05 - val_acc: 0.9111\n",
      "Epoch 332/600\n",
      " - 1s - loss: 4.3973e-04 - acc: 0.8258 - val_loss: 1.1658e-04 - val_acc: 0.9111\n",
      "Epoch 333/600\n",
      " - 1s - loss: 5.2418e-04 - acc: 0.8385 - val_loss: 1.5197e-04 - val_acc: 0.9056\n",
      "Epoch 334/600\n",
      " - 1s - loss: 5.5474e-04 - acc: 0.8298 - val_loss: 1.3867e-04 - val_acc: 0.9222\n",
      "Epoch 335/600\n",
      " - 1s - loss: 5.4883e-04 - acc: 0.8464 - val_loss: 3.7496e-05 - val_acc: 0.9222\n",
      "Epoch 336/600\n",
      " - 1s - loss: 4.5368e-04 - acc: 0.8373 - val_loss: 1.4914e-05 - val_acc: 0.9167\n",
      "Epoch 337/600\n",
      " - 1s - loss: 4.0750e-04 - acc: 0.8286 - val_loss: 5.5990e-05 - val_acc: 0.9278\n",
      "Epoch 338/600\n",
      " - 1s - loss: 4.8123e-04 - acc: 0.8345 - val_loss: 1.1419e-04 - val_acc: 0.9000\n",
      "Epoch 339/600\n",
      " - 1s - loss: 5.0910e-04 - acc: 0.8310 - val_loss: 8.4014e-05 - val_acc: 0.9222\n",
      "Epoch 340/600\n",
      " - 1s - loss: 4.7330e-04 - acc: 0.8337 - val_loss: 2.7386e-05 - val_acc: 0.9000\n",
      "Epoch 341/600\n",
      " - 1s - loss: 4.2844e-04 - acc: 0.8238 - val_loss: 6.5277e-06 - val_acc: 0.9056\n",
      "Epoch 342/600\n",
      " - 1s - loss: 4.0432e-04 - acc: 0.8389 - val_loss: 5.4251e-05 - val_acc: 0.9111\n",
      "Epoch 343/600\n",
      " - 1s - loss: 4.3659e-04 - acc: 0.8393 - val_loss: 6.8326e-05 - val_acc: 0.8944\n",
      "Epoch 344/600\n",
      " - 1s - loss: 4.6510e-04 - acc: 0.8250 - val_loss: 8.6036e-05 - val_acc: 0.9056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345/600\n",
      " - 1s - loss: 4.8218e-04 - acc: 0.8306 - val_loss: 4.9452e-05 - val_acc: 0.9111\n",
      "Epoch 346/600\n",
      " - 1s - loss: 4.3342e-04 - acc: 0.8294 - val_loss: 1.1277e-05 - val_acc: 0.9167\n",
      "Epoch 347/600\n",
      " - 1s - loss: 4.0194e-04 - acc: 0.8306 - val_loss: 1.4061e-05 - val_acc: 0.9167\n",
      "Epoch 348/600\n",
      " - 1s - loss: 4.0604e-04 - acc: 0.8321 - val_loss: 3.5507e-05 - val_acc: 0.9167\n",
      "Epoch 349/600\n",
      " - 1s - loss: 4.2370e-04 - acc: 0.8298 - val_loss: 1.1172e-04 - val_acc: 0.9167\n",
      "Epoch 350/600\n",
      " - 1s - loss: 4.8568e-04 - acc: 0.8361 - val_loss: 1.1862e-04 - val_acc: 0.9056\n",
      "Epoch 351/600\n",
      " - 1s - loss: 5.1548e-04 - acc: 0.8377 - val_loss: 8.4299e-05 - val_acc: 0.9167\n",
      "Epoch 352/600\n",
      " - 1s - loss: 4.7409e-04 - acc: 0.8317 - val_loss: 3.9915e-05 - val_acc: 0.9111\n",
      "Epoch 353/600\n",
      " - 1s - loss: 4.2181e-04 - acc: 0.8381 - val_loss: 2.2115e-05 - val_acc: 0.9167\n",
      "Epoch 354/600\n",
      " - 1s - loss: 4.0068e-04 - acc: 0.8385 - val_loss: 7.6387e-06 - val_acc: 0.9167\n",
      "Epoch 355/600\n",
      " - 1s - loss: 4.0829e-04 - acc: 0.8385 - val_loss: 7.5349e-06 - val_acc: 0.9111\n",
      "Epoch 356/600\n",
      " - 1s - loss: 3.9416e-04 - acc: 0.8405 - val_loss: 4.7248e-05 - val_acc: 0.9111\n",
      "Epoch 357/600\n",
      " - 1s - loss: 4.3381e-04 - acc: 0.8325 - val_loss: 7.4447e-05 - val_acc: 0.8944\n",
      "Epoch 358/600\n",
      " - 1s - loss: 4.8000e-04 - acc: 0.8349 - val_loss: 1.6910e-04 - val_acc: 0.9056\n",
      "Epoch 359/600\n",
      " - 1s - loss: 5.2705e-04 - acc: 0.8290 - val_loss: 2.0644e-04 - val_acc: 0.8944\n",
      "Epoch 360/600\n",
      " - 1s - loss: 5.8403e-04 - acc: 0.8294 - val_loss: 2.4673e-04 - val_acc: 0.9222\n",
      "Epoch 361/600\n",
      " - 1s - loss: 6.0398e-04 - acc: 0.8488 - val_loss: 2.0018e-04 - val_acc: 0.8778\n",
      "Epoch 362/600\n",
      " - 1s - loss: 5.9507e-04 - acc: 0.8286 - val_loss: 2.3091e-04 - val_acc: 0.9111\n",
      "Epoch 363/600\n",
      " - 1s - loss: 6.1834e-04 - acc: 0.8317 - val_loss: 1.9940e-04 - val_acc: 0.8889\n",
      "Epoch 364/600\n",
      " - 1s - loss: 5.9159e-04 - acc: 0.8286 - val_loss: 2.5592e-04 - val_acc: 0.9056\n",
      "Epoch 365/600\n",
      " - 1s - loss: 6.4331e-04 - acc: 0.8437 - val_loss: 3.6374e-04 - val_acc: 0.9000\n",
      "Epoch 366/600\n",
      " - 1s - loss: 7.4982e-04 - acc: 0.8214 - val_loss: 6.1054e-04 - val_acc: 0.9111\n",
      "Epoch 367/600\n",
      " - 1s - loss: 9.7519e-04 - acc: 0.8349 - val_loss: 7.3710e-04 - val_acc: 0.8944\n",
      "Epoch 368/600\n",
      " - 1s - loss: 0.0011 - acc: 0.8317 - val_loss: 9.2193e-04 - val_acc: 0.9111\n",
      "Epoch 369/600\n",
      " - 1s - loss: 0.0013 - acc: 0.8433 - val_loss: 7.8040e-04 - val_acc: 0.8889\n",
      "Epoch 370/600\n",
      " - 1s - loss: 0.0011 - acc: 0.8230 - val_loss: 5.4410e-04 - val_acc: 0.9056\n",
      "Epoch 371/600\n",
      " - 1s - loss: 9.2610e-04 - acc: 0.8464 - val_loss: 2.4650e-04 - val_acc: 0.8889\n",
      "Epoch 372/600\n",
      " - 1s - loss: 6.3597e-04 - acc: 0.8290 - val_loss: 1.0201e-04 - val_acc: 0.9056\n",
      "Epoch 373/600\n",
      " - 1s - loss: 4.7221e-04 - acc: 0.8353 - val_loss: 1.0760e-05 - val_acc: 0.8944\n",
      "Epoch 374/600\n",
      " - 1s - loss: 3.8652e-04 - acc: 0.8321 - val_loss: 3.1057e-05 - val_acc: 0.9000\n",
      "Epoch 375/600\n",
      " - 1s - loss: 4.1409e-04 - acc: 0.8405 - val_loss: 1.0561e-04 - val_acc: 0.9111\n",
      "Epoch 376/600\n",
      " - 1s - loss: 4.9793e-04 - acc: 0.8373 - val_loss: 1.7950e-04 - val_acc: 0.8889\n",
      "Epoch 377/600\n",
      " - 1s - loss: 5.7784e-04 - acc: 0.8167 - val_loss: 3.3680e-04 - val_acc: 0.9056\n",
      "Epoch 378/600\n",
      " - 1s - loss: 6.7821e-04 - acc: 0.8377 - val_loss: 2.9501e-04 - val_acc: 0.8944\n",
      "Epoch 379/600\n",
      " - 1s - loss: 7.0952e-04 - acc: 0.8250 - val_loss: 3.4910e-04 - val_acc: 0.9111\n",
      "Epoch 380/600\n",
      " - 1s - loss: 6.8931e-04 - acc: 0.8365 - val_loss: 2.2051e-04 - val_acc: 0.9000\n",
      "Epoch 381/600\n",
      " - 1s - loss: 6.1652e-04 - acc: 0.8310 - val_loss: 1.6313e-04 - val_acc: 0.9167\n",
      "Epoch 382/600\n",
      " - 1s - loss: 5.3745e-04 - acc: 0.8429 - val_loss: 5.5265e-05 - val_acc: 0.9111\n",
      "Epoch 383/600\n",
      " - 1s - loss: 4.4259e-04 - acc: 0.8413 - val_loss: 2.5390e-05 - val_acc: 0.9167\n",
      "Epoch 384/600\n",
      " - 1s - loss: 3.8624e-04 - acc: 0.8353 - val_loss: 2.0691e-05 - val_acc: 0.9056\n",
      "Epoch 385/600\n",
      " - 1s - loss: 3.6676e-04 - acc: 0.8353 - val_loss: 7.7618e-05 - val_acc: 0.9000\n",
      "Epoch 386/600\n",
      " - 1s - loss: 4.4851e-04 - acc: 0.8286 - val_loss: 1.9423e-04 - val_acc: 0.9111\n",
      "Epoch 387/600\n",
      " - 1s - loss: 5.7588e-04 - acc: 0.8425 - val_loss: 3.2670e-04 - val_acc: 0.8889\n",
      "Epoch 388/600\n",
      " - 1s - loss: 7.0907e-04 - acc: 0.8278 - val_loss: 4.3958e-04 - val_acc: 0.9000\n",
      "Epoch 389/600\n",
      " - 1s - loss: 7.8320e-04 - acc: 0.8385 - val_loss: 3.5018e-04 - val_acc: 0.8778\n",
      "Epoch 390/600\n",
      " - 1s - loss: 7.3110e-04 - acc: 0.8234 - val_loss: 2.5838e-04 - val_acc: 0.9000\n",
      "Epoch 391/600\n",
      " - 1s - loss: 5.9938e-04 - acc: 0.8357 - val_loss: 7.1841e-05 - val_acc: 0.9167\n",
      "Epoch 392/600\n",
      " - 1s - loss: 4.5364e-04 - acc: 0.8341 - val_loss: 1.0083e-05 - val_acc: 0.9111\n",
      "Epoch 393/600\n",
      " - 1s - loss: 3.7916e-04 - acc: 0.8389 - val_loss: 2.8438e-05 - val_acc: 0.9111\n",
      "Epoch 394/600\n",
      " - 1s - loss: 3.9048e-04 - acc: 0.8369 - val_loss: 7.7713e-05 - val_acc: 0.9056\n",
      "Epoch 395/600\n",
      " - 1s - loss: 4.4663e-04 - acc: 0.8262 - val_loss: 2.3020e-04 - val_acc: 0.9222\n",
      "Epoch 396/600\n",
      " - 1s - loss: 5.7483e-04 - acc: 0.8433 - val_loss: 2.7920e-04 - val_acc: 0.8778\n",
      "Epoch 397/600\n",
      " - 1s - loss: 6.5621e-04 - acc: 0.8262 - val_loss: 3.5115e-04 - val_acc: 0.8944\n",
      "Epoch 398/600\n",
      " - 1s - loss: 6.7796e-04 - acc: 0.8437 - val_loss: 2.5880e-04 - val_acc: 0.8889\n",
      "Epoch 399/600\n",
      " - 1s - loss: 6.1827e-04 - acc: 0.8242 - val_loss: 1.7726e-04 - val_acc: 0.8944\n",
      "Epoch 400/600\n",
      " - 1s - loss: 5.2054e-04 - acc: 0.8413 - val_loss: 6.6861e-05 - val_acc: 0.9056\n",
      "Epoch 401/600\n",
      " - 1s - loss: 4.2595e-04 - acc: 0.8365 - val_loss: 1.4026e-05 - val_acc: 0.9167\n",
      "Epoch 402/600\n",
      " - 1s - loss: 3.7653e-04 - acc: 0.8385 - val_loss: 3.3300e-05 - val_acc: 0.9278\n",
      "Epoch 403/600\n",
      " - 1s - loss: 4.0871e-04 - acc: 0.8381 - val_loss: 8.2039e-05 - val_acc: 0.9167\n",
      "Epoch 404/600\n",
      " - 1s - loss: 4.4509e-04 - acc: 0.8341 - val_loss: 2.1570e-04 - val_acc: 0.9111\n",
      "Epoch 405/600\n",
      " - 1s - loss: 5.6060e-04 - acc: 0.8425 - val_loss: 3.0132e-04 - val_acc: 0.8944\n",
      "Epoch 406/600\n",
      " - 1s - loss: 6.6470e-04 - acc: 0.8286 - val_loss: 4.2656e-04 - val_acc: 0.9000\n",
      "Epoch 407/600\n",
      " - 1s - loss: 7.8334e-04 - acc: 0.8512 - val_loss: 4.4399e-04 - val_acc: 0.8722\n",
      "Epoch 408/600\n",
      " - 1s - loss: 8.1177e-04 - acc: 0.8218 - val_loss: 5.1545e-04 - val_acc: 0.8889\n",
      "Epoch 409/600\n",
      " - 1s - loss: 8.5837e-04 - acc: 0.8341 - val_loss: 4.6212e-04 - val_acc: 0.8778\n",
      "Epoch 410/600\n",
      " - 1s - loss: 8.4786e-04 - acc: 0.8230 - val_loss: 4.7216e-04 - val_acc: 0.9111\n",
      "Epoch 411/600\n",
      " - 1s - loss: 7.8891e-04 - acc: 0.8492 - val_loss: 2.5396e-04 - val_acc: 0.8944\n",
      "Epoch 412/600\n",
      " - 1s - loss: 6.0566e-04 - acc: 0.8365 - val_loss: 1.2115e-04 - val_acc: 0.9222\n",
      "Epoch 413/600\n",
      " - 1s - loss: 4.7682e-04 - acc: 0.8353 - val_loss: 2.4353e-05 - val_acc: 0.9167\n",
      "Epoch 414/600\n",
      " - 1s - loss: 3.9119e-04 - acc: 0.8361 - val_loss: 2.0377e-05 - val_acc: 0.9056\n",
      "Epoch 415/600\n",
      " - 1s - loss: 3.7012e-04 - acc: 0.8460 - val_loss: 5.2838e-05 - val_acc: 0.8944\n",
      "Epoch 416/600\n",
      " - 1s - loss: 3.8802e-04 - acc: 0.8425 - val_loss: 3.3841e-05 - val_acc: 0.8944\n",
      "Epoch 417/600\n",
      " - 1s - loss: 3.8059e-04 - acc: 0.8329 - val_loss: 6.3994e-05 - val_acc: 0.8944\n",
      "Epoch 418/600\n",
      " - 1s - loss: 4.0128e-04 - acc: 0.8433 - val_loss: 2.9651e-05 - val_acc: 0.9056\n",
      "Epoch 419/600\n",
      " - 1s - loss: 3.6722e-04 - acc: 0.8353 - val_loss: 1.9693e-05 - val_acc: 0.9111\n",
      "Epoch 420/600\n",
      " - 1s - loss: 3.6122e-04 - acc: 0.8349 - val_loss: 1.5682e-05 - val_acc: 0.9111\n",
      "Epoch 421/600\n",
      " - 1s - loss: 3.6005e-04 - acc: 0.8452 - val_loss: 3.6326e-05 - val_acc: 0.9111\n",
      "Epoch 422/600\n",
      " - 1s - loss: 3.8875e-04 - acc: 0.8377 - val_loss: 3.0185e-05 - val_acc: 0.9111\n",
      "Epoch 423/600\n",
      " - 1s - loss: 3.8230e-04 - acc: 0.8437 - val_loss: 5.4123e-05 - val_acc: 0.9111\n",
      "Epoch 424/600\n",
      " - 1s - loss: 4.0815e-04 - acc: 0.8393 - val_loss: 1.5484e-04 - val_acc: 0.9167\n",
      "Epoch 425/600\n",
      " - 1s - loss: 4.5847e-04 - acc: 0.8480 - val_loss: 1.9141e-04 - val_acc: 0.8833\n",
      "Epoch 426/600\n",
      " - 1s - loss: 5.5738e-04 - acc: 0.8266 - val_loss: 3.4737e-04 - val_acc: 0.9000\n",
      "Epoch 427/600\n",
      " - 1s - loss: 6.6942e-04 - acc: 0.8413 - val_loss: 4.5586e-04 - val_acc: 0.8667\n",
      "Epoch 428/600\n",
      " - 1s - loss: 8.1389e-04 - acc: 0.8425 - val_loss: 6.7920e-04 - val_acc: 0.8944\n",
      "Epoch 429/600\n",
      " - 1s - loss: 0.0010 - acc: 0.8508 - val_loss: 7.8175e-04 - val_acc: 0.8778\n",
      "Epoch 430/600\n",
      " - 1s - loss: 0.0012 - acc: 0.8234 - val_loss: 0.0011 - val_acc: 0.9056\n",
      "Epoch 431/600\n",
      " - 1s - loss: 0.0014 - acc: 0.8440 - val_loss: 0.0011 - val_acc: 0.8778\n",
      "Epoch 432/600\n",
      " - 1s - loss: 0.0014 - acc: 0.8171 - val_loss: 0.0011 - val_acc: 0.8944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 433/600\n",
      " - 1s - loss: 0.0013 - acc: 0.8409 - val_loss: 5.8362e-04 - val_acc: 0.8833\n",
      "Epoch 434/600\n",
      " - 1s - loss: 9.7029e-04 - acc: 0.8278 - val_loss: 3.4314e-04 - val_acc: 0.8944\n",
      "Epoch 435/600\n",
      " - 1s - loss: 6.7809e-04 - acc: 0.8286 - val_loss: 9.7999e-05 - val_acc: 0.8778\n",
      "Epoch 436/600\n",
      " - 1s - loss: 4.2209e-04 - acc: 0.8365 - val_loss: 1.8103e-05 - val_acc: 0.8944\n",
      "Epoch 437/600\n",
      " - 1s - loss: 3.6399e-04 - acc: 0.8337 - val_loss: 1.0291e-04 - val_acc: 0.9000\n",
      "Epoch 438/600\n",
      " - 1s - loss: 4.3475e-04 - acc: 0.8365 - val_loss: 2.7576e-04 - val_acc: 0.8778\n",
      "Epoch 439/600\n",
      " - 1s - loss: 6.2209e-04 - acc: 0.8250 - val_loss: 4.2959e-04 - val_acc: 0.9111\n",
      "Epoch 440/600\n",
      " - 1s - loss: 7.4444e-04 - acc: 0.8444 - val_loss: 3.7665e-04 - val_acc: 0.8778\n",
      "Epoch 441/600\n",
      " - 1s - loss: 7.2883e-04 - acc: 0.8230 - val_loss: 2.3299e-04 - val_acc: 0.8944\n",
      "Epoch 442/600\n",
      " - 1s - loss: 5.7314e-04 - acc: 0.8413 - val_loss: 6.7496e-05 - val_acc: 0.8944\n",
      "Epoch 443/600\n",
      " - 1s - loss: 4.1723e-04 - acc: 0.8302 - val_loss: 2.8430e-05 - val_acc: 0.8944\n",
      "Epoch 444/600\n",
      " - 1s - loss: 3.6221e-04 - acc: 0.8353 - val_loss: 2.5542e-04 - val_acc: 0.9056\n",
      "Epoch 445/600\n",
      " - 1s - loss: 5.7987e-04 - acc: 0.8444 - val_loss: 4.2939e-04 - val_acc: 0.8778\n",
      "Epoch 446/600\n",
      " - 1s - loss: 7.8064e-04 - acc: 0.8290 - val_loss: 6.1015e-04 - val_acc: 0.9167\n",
      "Epoch 447/600\n",
      " - 1s - loss: 9.3787e-04 - acc: 0.8405 - val_loss: 4.1679e-04 - val_acc: 0.8833\n",
      "Epoch 448/600\n",
      " - 1s - loss: 7.7739e-04 - acc: 0.8306 - val_loss: 1.9788e-04 - val_acc: 0.9111\n",
      "Epoch 449/600\n",
      " - 1s - loss: 5.1383e-04 - acc: 0.8528 - val_loss: 1.0405e-05 - val_acc: 0.9000\n",
      "Epoch 450/600\n",
      " - 1s - loss: 3.3464e-04 - acc: 0.8472 - val_loss: 1.1515e-04 - val_acc: 0.8833\n",
      "Epoch 451/600\n",
      " - 1s - loss: 4.7210e-04 - acc: 0.8353 - val_loss: 3.5688e-04 - val_acc: 0.9000\n",
      "Epoch 452/600\n",
      " - 1s - loss: 6.9838e-04 - acc: 0.8393 - val_loss: 5.0646e-04 - val_acc: 0.8778\n",
      "Epoch 453/600\n",
      " - 1s - loss: 8.2631e-04 - acc: 0.8361 - val_loss: 4.7239e-04 - val_acc: 0.9222\n",
      "Epoch 454/600\n",
      " - 1s - loss: 7.8582e-04 - acc: 0.8357 - val_loss: 2.1780e-04 - val_acc: 0.8833\n",
      "Epoch 455/600\n",
      " - 1s - loss: 5.7329e-04 - acc: 0.8302 - val_loss: 5.9199e-05 - val_acc: 0.9000\n",
      "Epoch 456/600\n",
      " - 1s - loss: 3.8936e-04 - acc: 0.8389 - val_loss: 3.4312e-05 - val_acc: 0.9000\n",
      "Epoch 457/600\n",
      " - 1s - loss: 3.6065e-04 - acc: 0.8448 - val_loss: 1.0722e-04 - val_acc: 0.8833\n",
      "Epoch 458/600\n",
      " - 1s - loss: 4.2859e-04 - acc: 0.8365 - val_loss: 1.4034e-04 - val_acc: 0.9056\n",
      "Epoch 459/600\n",
      " - 1s - loss: 4.8125e-04 - acc: 0.8381 - val_loss: 1.8057e-04 - val_acc: 0.8944\n",
      "Epoch 460/600\n",
      " - 1s - loss: 5.2975e-04 - acc: 0.8448 - val_loss: 1.7911e-04 - val_acc: 0.9167\n",
      "Epoch 461/600\n",
      " - 1s - loss: 4.9345e-04 - acc: 0.8413 - val_loss: 1.1965e-04 - val_acc: 0.8944\n",
      "Epoch 462/600\n",
      " - 1s - loss: 4.6256e-04 - acc: 0.8250 - val_loss: 4.1611e-05 - val_acc: 0.9167\n",
      "Epoch 463/600\n",
      " - 1s - loss: 3.5662e-04 - acc: 0.8413 - val_loss: 3.3323e-05 - val_acc: 0.9111\n",
      "Epoch 464/600\n",
      " - 1s - loss: 3.6854e-04 - acc: 0.8429 - val_loss: 8.1058e-05 - val_acc: 0.8944\n",
      "Epoch 465/600\n",
      " - 1s - loss: 3.9617e-04 - acc: 0.8357 - val_loss: 2.4446e-04 - val_acc: 0.8944\n",
      "Epoch 466/600\n",
      " - 1s - loss: 5.7273e-04 - acc: 0.8369 - val_loss: 3.8327e-04 - val_acc: 0.8722\n",
      "Epoch 467/600\n",
      " - 1s - loss: 7.1997e-04 - acc: 0.8278 - val_loss: 4.5530e-04 - val_acc: 0.8889\n",
      "Epoch 468/600\n",
      " - 1s - loss: 8.0684e-04 - acc: 0.8425 - val_loss: 3.5762e-04 - val_acc: 0.8889\n",
      "Epoch 469/600\n",
      " - 1s - loss: 6.8952e-04 - acc: 0.8266 - val_loss: 2.2586e-04 - val_acc: 0.9056\n",
      "Epoch 470/600\n",
      " - 1s - loss: 5.3623e-04 - acc: 0.8397 - val_loss: 4.7649e-05 - val_acc: 0.9056\n",
      "Epoch 471/600\n",
      " - 1s - loss: 3.5423e-04 - acc: 0.8456 - val_loss: 2.5153e-05 - val_acc: 0.9167\n",
      "Epoch 472/600\n",
      " - 1s - loss: 3.6107e-04 - acc: 0.8421 - val_loss: 1.7670e-04 - val_acc: 0.9222\n",
      "Epoch 473/600\n",
      " - 1s - loss: 5.1169e-04 - acc: 0.8417 - val_loss: 4.8811e-04 - val_acc: 0.8833\n",
      "Epoch 474/600\n",
      " - 1s - loss: 8.0150e-04 - acc: 0.8317 - val_loss: 7.1609e-04 - val_acc: 0.9167\n",
      "Epoch 475/600\n",
      " - 1s - loss: 0.0010 - acc: 0.8417 - val_loss: 6.4861e-04 - val_acc: 0.8778\n",
      "Epoch 476/600\n",
      " - 1s - loss: 0.0010 - acc: 0.8274 - val_loss: 4.8687e-04 - val_acc: 0.8944\n",
      "Epoch 477/600\n",
      " - 1s - loss: 7.7587e-04 - acc: 0.8437 - val_loss: 1.2781e-04 - val_acc: 0.8944\n",
      "Epoch 478/600\n",
      " - 1s - loss: 4.4741e-04 - acc: 0.8361 - val_loss: 2.6595e-05 - val_acc: 0.9056\n",
      "Epoch 479/600\n",
      " - 1s - loss: 3.6601e-04 - acc: 0.8353 - val_loss: 2.9779e-04 - val_acc: 0.8944\n",
      "Epoch 480/600\n",
      " - 1s - loss: 6.1228e-04 - acc: 0.8413 - val_loss: 5.5507e-04 - val_acc: 0.8778\n",
      "Epoch 481/600\n",
      " - 1s - loss: 8.7758e-04 - acc: 0.8190 - val_loss: 6.7219e-04 - val_acc: 0.9167\n",
      "Epoch 482/600\n",
      " - 1s - loss: 9.7155e-04 - acc: 0.8528 - val_loss: 4.5558e-04 - val_acc: 0.8944\n",
      "Epoch 483/600\n",
      " - 1s - loss: 7.7607e-04 - acc: 0.8333 - val_loss: 1.7159e-04 - val_acc: 0.9111\n",
      "Epoch 484/600\n",
      " - 1s - loss: 4.8218e-04 - acc: 0.8472 - val_loss: 1.7585e-05 - val_acc: 0.9000\n",
      "Epoch 485/600\n",
      " - 1s - loss: 3.3277e-04 - acc: 0.8421 - val_loss: 1.5366e-04 - val_acc: 0.9000\n",
      "Epoch 486/600\n",
      " - 1s - loss: 4.6797e-04 - acc: 0.8321 - val_loss: 3.5742e-04 - val_acc: 0.9111\n",
      "Epoch 487/600\n",
      " - 1s - loss: 6.5965e-04 - acc: 0.8421 - val_loss: 3.6344e-04 - val_acc: 0.8889\n",
      "Epoch 488/600\n",
      " - 1s - loss: 6.7401e-04 - acc: 0.8365 - val_loss: 2.1317e-04 - val_acc: 0.9333\n",
      "Epoch 489/600\n",
      " - 1s - loss: 5.1364e-04 - acc: 0.8512 - val_loss: 3.2695e-05 - val_acc: 0.9000\n",
      "Epoch 490/600\n",
      " - 1s - loss: 3.4871e-04 - acc: 0.8413 - val_loss: 1.5529e-05 - val_acc: 0.9111\n",
      "Epoch 491/600\n",
      " - 1s - loss: 3.1585e-04 - acc: 0.8488 - val_loss: 7.5073e-05 - val_acc: 0.9000\n",
      "Epoch 492/600\n",
      " - 1s - loss: 3.9332e-04 - acc: 0.8433 - val_loss: 1.2343e-04 - val_acc: 0.8889\n",
      "Epoch 493/600\n",
      " - 1s - loss: 4.4121e-04 - acc: 0.8262 - val_loss: 1.6735e-04 - val_acc: 0.8944\n",
      "Epoch 494/600\n",
      " - 1s - loss: 4.7307e-04 - acc: 0.8448 - val_loss: 1.1511e-04 - val_acc: 0.9000\n",
      "Epoch 495/600\n",
      " - 1s - loss: 4.2528e-04 - acc: 0.8377 - val_loss: 5.1682e-05 - val_acc: 0.9056\n",
      "Epoch 496/600\n",
      " - 1s - loss: 3.7159e-04 - acc: 0.8429 - val_loss: 1.3335e-05 - val_acc: 0.9056\n",
      "Epoch 497/600\n",
      " - 1s - loss: 3.2293e-04 - acc: 0.8448 - val_loss: 1.6349e-05 - val_acc: 0.9000\n",
      "Epoch 498/600\n",
      " - 1s - loss: 3.2246e-04 - acc: 0.8413 - val_loss: 7.4568e-05 - val_acc: 0.9278\n",
      "Epoch 499/600\n",
      " - 1s - loss: 3.7363e-04 - acc: 0.8480 - val_loss: 1.5278e-04 - val_acc: 0.9000\n",
      "Epoch 500/600\n",
      " - 1s - loss: 4.5534e-04 - acc: 0.8405 - val_loss: 1.9198e-04 - val_acc: 0.9222\n",
      "Epoch 501/600\n",
      " - 1s - loss: 4.9147e-04 - acc: 0.8397 - val_loss: 1.4824e-04 - val_acc: 0.8944\n",
      "Epoch 502/600\n",
      " - 1s - loss: 4.6647e-04 - acc: 0.8381 - val_loss: 1.2714e-04 - val_acc: 0.9056\n",
      "Epoch 503/600\n",
      " - 1s - loss: 4.4106e-04 - acc: 0.8321 - val_loss: 7.6183e-05 - val_acc: 0.8944\n",
      "Epoch 504/600\n",
      " - 1s - loss: 3.9166e-04 - acc: 0.8456 - val_loss: 4.2262e-05 - val_acc: 0.8944\n",
      "Epoch 505/600\n",
      " - 1s - loss: 3.4568e-04 - acc: 0.8353 - val_loss: 7.4839e-06 - val_acc: 0.9000\n",
      "Epoch 506/600\n",
      " - 1s - loss: 3.1845e-04 - acc: 0.8369 - val_loss: 4.2346e-05 - val_acc: 0.9056\n",
      "Epoch 507/600\n",
      " - 1s - loss: 3.2983e-04 - acc: 0.8333 - val_loss: 1.2175e-04 - val_acc: 0.9167\n",
      "Epoch 508/600\n",
      " - 1s - loss: 4.3425e-04 - acc: 0.8460 - val_loss: 1.8781e-04 - val_acc: 0.8944\n",
      "Epoch 509/600\n",
      " - 1s - loss: 4.7687e-04 - acc: 0.8369 - val_loss: 2.2948e-04 - val_acc: 0.9111\n",
      "Epoch 510/600\n",
      " - 1s - loss: 5.3080e-04 - acc: 0.8401 - val_loss: 1.7210e-04 - val_acc: 0.8833\n",
      "Epoch 511/600\n",
      " - 1s - loss: 4.8911e-04 - acc: 0.8433 - val_loss: 1.5065e-04 - val_acc: 0.9000\n",
      "Epoch 512/600\n",
      " - 1s - loss: 4.4539e-04 - acc: 0.8448 - val_loss: 6.5247e-05 - val_acc: 0.9000\n",
      "Epoch 513/600\n",
      " - 1s - loss: 3.6979e-04 - acc: 0.8313 - val_loss: 3.6021e-05 - val_acc: 0.9222\n",
      "Epoch 514/600\n",
      " - 1s - loss: 3.3086e-04 - acc: 0.8385 - val_loss: 7.0100e-06 - val_acc: 0.9000\n",
      "Epoch 515/600\n",
      " - 1s - loss: 3.0082e-04 - acc: 0.8393 - val_loss: 1.4649e-05 - val_acc: 0.9056\n",
      "Epoch 516/600\n",
      " - 1s - loss: 3.2456e-04 - acc: 0.8425 - val_loss: 6.1553e-05 - val_acc: 0.9056\n",
      "Epoch 517/600\n",
      " - 1s - loss: 3.4084e-04 - acc: 0.8460 - val_loss: 1.4179e-04 - val_acc: 0.8889\n",
      "Epoch 518/600\n",
      " - 1s - loss: 4.4111e-04 - acc: 0.8433 - val_loss: 2.5506e-04 - val_acc: 0.9000\n",
      "Epoch 519/600\n",
      " - 1s - loss: 5.5758e-04 - acc: 0.8520 - val_loss: 2.7587e-04 - val_acc: 0.8833\n",
      "Epoch 520/600\n",
      " - 1s - loss: 5.8324e-04 - acc: 0.8310 - val_loss: 3.0034e-04 - val_acc: 0.8889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 521/600\n",
      " - 1s - loss: 5.5549e-04 - acc: 0.8349 - val_loss: 1.6964e-04 - val_acc: 0.8944\n",
      "Epoch 522/600\n",
      " - 1s - loss: 4.7522e-04 - acc: 0.8349 - val_loss: 1.1576e-04 - val_acc: 0.9222\n",
      "Epoch 523/600\n",
      " - 1s - loss: 4.1492e-04 - acc: 0.8421 - val_loss: 9.7757e-05 - val_acc: 0.9056\n",
      "Epoch 524/600\n",
      " - 1s - loss: 3.8910e-04 - acc: 0.8421 - val_loss: 8.6692e-05 - val_acc: 0.9111\n",
      "Epoch 525/600\n",
      " - 1s - loss: 3.5256e-04 - acc: 0.8377 - val_loss: 4.7197e-05 - val_acc: 0.9000\n",
      "Epoch 526/600\n",
      " - 1s - loss: 3.2534e-04 - acc: 0.8433 - val_loss: 1.7828e-05 - val_acc: 0.9056\n",
      "Epoch 527/600\n",
      " - 1s - loss: 3.0193e-04 - acc: 0.8456 - val_loss: 1.6906e-05 - val_acc: 0.9000\n",
      "Epoch 528/600\n",
      " - 1s - loss: 2.9917e-04 - acc: 0.8421 - val_loss: 1.2554e-05 - val_acc: 0.9056\n",
      "Epoch 529/600\n",
      " - 1s - loss: 2.9552e-04 - acc: 0.8528 - val_loss: 8.4663e-06 - val_acc: 0.9056\n",
      "Epoch 530/600\n",
      " - 1s - loss: 3.0586e-04 - acc: 0.8389 - val_loss: 5.7191e-06 - val_acc: 0.9056\n",
      "Epoch 531/600\n",
      " - 1s - loss: 2.9995e-04 - acc: 0.8369 - val_loss: 1.3116e-05 - val_acc: 0.9111\n",
      "Epoch 532/600\n",
      " - 1s - loss: 2.9966e-04 - acc: 0.8452 - val_loss: 3.0276e-05 - val_acc: 0.9000\n",
      "Epoch 533/600\n",
      " - 1s - loss: 3.2151e-04 - acc: 0.8306 - val_loss: 7.9672e-05 - val_acc: 0.9222\n",
      "Epoch 534/600\n",
      " - 1s - loss: 3.5368e-04 - acc: 0.8421 - val_loss: 1.4151e-04 - val_acc: 0.8944\n",
      "Epoch 535/600\n",
      " - 1s - loss: 4.3801e-04 - acc: 0.8317 - val_loss: 3.8305e-04 - val_acc: 0.9000\n",
      "Epoch 536/600\n",
      " - 1s - loss: 6.4971e-04 - acc: 0.8456 - val_loss: 6.5871e-04 - val_acc: 0.8611\n",
      "Epoch 537/600\n",
      " - 1s - loss: 9.5533e-04 - acc: 0.8353 - val_loss: 0.0012 - val_acc: 0.8944\n",
      "Epoch 538/600\n",
      " - 1s - loss: 0.0015 - acc: 0.8464 - val_loss: 0.0018 - val_acc: 0.8611\n",
      "Epoch 539/600\n",
      " - 1s - loss: 0.0021 - acc: 0.8226 - val_loss: 0.0028 - val_acc: 0.8611\n",
      "Epoch 540/600\n",
      " - 1s - loss: 0.0030 - acc: 0.8401 - val_loss: 0.0029 - val_acc: 0.8500\n",
      "Epoch 541/600\n",
      " - 1s - loss: 0.0033 - acc: 0.8139 - val_loss: 0.0026 - val_acc: 0.8611\n",
      "Epoch 542/600\n",
      " - 1s - loss: 0.0029 - acc: 0.8337 - val_loss: 0.0011 - val_acc: 0.8611\n",
      "Epoch 543/600\n",
      " - 1s - loss: 0.0014 - acc: 0.8325 - val_loss: 1.0234e-04 - val_acc: 0.8778\n",
      "Epoch 544/600\n",
      " - 1s - loss: 3.8491e-04 - acc: 0.8238 - val_loss: 4.2157e-04 - val_acc: 0.8778\n",
      "Epoch 545/600\n",
      " - 1s - loss: 6.9980e-04 - acc: 0.8341 - val_loss: 0.0014 - val_acc: 0.8500\n",
      "Epoch 546/600\n",
      " - 1s - loss: 0.0017 - acc: 0.8103 - val_loss: 0.0019 - val_acc: 0.9000\n",
      "Epoch 547/600\n",
      " - 1s - loss: 0.0021 - acc: 0.8504 - val_loss: 9.9530e-04 - val_acc: 0.8444\n",
      "Epoch 548/600\n",
      " - 1s - loss: 0.0013 - acc: 0.8171 - val_loss: 1.1764e-04 - val_acc: 0.8944\n",
      "Epoch 549/600\n",
      " - 1s - loss: 4.1585e-04 - acc: 0.8460 - val_loss: 3.8520e-04 - val_acc: 0.8778\n",
      "Epoch 550/600\n",
      " - 1s - loss: 6.6455e-04 - acc: 0.8464 - val_loss: 9.8572e-04 - val_acc: 0.8556\n",
      "Epoch 551/600\n",
      " - 1s - loss: 0.0013 - acc: 0.8266 - val_loss: 8.0280e-04 - val_acc: 0.8833\n",
      "Epoch 552/600\n",
      " - 1s - loss: 0.0011 - acc: 0.8385 - val_loss: 9.5186e-05 - val_acc: 0.9000\n",
      "Epoch 553/600\n",
      " - 1s - loss: 4.1005e-04 - acc: 0.8460 - val_loss: 1.7432e-04 - val_acc: 0.8944\n",
      "Epoch 554/600\n",
      " - 1s - loss: 4.8001e-04 - acc: 0.8349 - val_loss: 5.8258e-04 - val_acc: 0.8944\n",
      "Epoch 555/600\n",
      " - 1s - loss: 8.9814e-04 - acc: 0.8508 - val_loss: 3.7758e-04 - val_acc: 0.8833\n",
      "Epoch 556/600\n",
      " - 1s - loss: 6.9705e-04 - acc: 0.8405 - val_loss: 5.0279e-05 - val_acc: 0.8889\n",
      "Epoch 557/600\n",
      " - 1s - loss: 3.5431e-04 - acc: 0.8437 - val_loss: 1.7922e-04 - val_acc: 0.8778\n",
      "Epoch 558/600\n",
      " - 1s - loss: 4.6616e-04 - acc: 0.8357 - val_loss: 3.8617e-04 - val_acc: 0.8667\n",
      "Epoch 559/600\n",
      " - 1s - loss: 6.9750e-04 - acc: 0.8365 - val_loss: 2.0369e-04 - val_acc: 0.9167\n",
      "Epoch 560/600\n",
      " - 1s - loss: 4.9220e-04 - acc: 0.8548 - val_loss: 1.7053e-05 - val_acc: 0.9056\n",
      "Epoch 561/600\n",
      " - 1s - loss: 3.3114e-04 - acc: 0.8397 - val_loss: 1.2529e-04 - val_acc: 0.9000\n",
      "Epoch 562/600\n",
      " - 1s - loss: 4.3656e-04 - acc: 0.8444 - val_loss: 2.0702e-04 - val_acc: 0.8944\n",
      "Epoch 563/600\n",
      " - 1s - loss: 4.9831e-04 - acc: 0.8353 - val_loss: 7.2068e-05 - val_acc: 0.8889\n",
      "Epoch 564/600\n",
      " - 1s - loss: 3.8781e-04 - acc: 0.8294 - val_loss: 1.3939e-05 - val_acc: 0.8833\n",
      "Epoch 565/600\n",
      " - 1s - loss: 3.1714e-04 - acc: 0.8409 - val_loss: 8.7883e-05 - val_acc: 0.8833\n",
      "Epoch 566/600\n",
      " - 1s - loss: 3.8019e-04 - acc: 0.8397 - val_loss: 9.0814e-05 - val_acc: 0.8944\n",
      "Epoch 567/600\n",
      " - 1s - loss: 3.9069e-04 - acc: 0.8452 - val_loss: 2.1881e-05 - val_acc: 0.9000\n",
      "Epoch 568/600\n",
      " - 1s - loss: 3.1646e-04 - acc: 0.8464 - val_loss: 2.3482e-05 - val_acc: 0.9111\n",
      "Epoch 569/600\n",
      " - 1s - loss: 3.2533e-04 - acc: 0.8468 - val_loss: 4.8465e-05 - val_acc: 0.9167\n",
      "Epoch 570/600\n",
      " - 1s - loss: 3.6081e-04 - acc: 0.8429 - val_loss: 5.9146e-05 - val_acc: 0.9167\n",
      "Epoch 571/600\n",
      " - 1s - loss: 3.4397e-04 - acc: 0.8456 - val_loss: 1.3411e-05 - val_acc: 0.9222\n",
      "Epoch 572/600\n",
      " - 1s - loss: 3.1485e-04 - acc: 0.8333 - val_loss: 1.7320e-05 - val_acc: 0.9111\n",
      "Epoch 573/600\n",
      " - 1s - loss: 3.0970e-04 - acc: 0.8496 - val_loss: 4.7382e-05 - val_acc: 0.9056\n",
      "Epoch 574/600\n",
      " - 1s - loss: 3.2805e-04 - acc: 0.8429 - val_loss: 2.7357e-05 - val_acc: 0.9000\n",
      "Epoch 575/600\n",
      " - 1s - loss: 3.2424e-04 - acc: 0.8337 - val_loss: 1.1873e-05 - val_acc: 0.8944\n",
      "Epoch 576/600\n",
      " - 1s - loss: 3.1327e-04 - acc: 0.8409 - val_loss: 4.1359e-05 - val_acc: 0.9000\n",
      "Epoch 577/600\n",
      " - 1s - loss: 3.3985e-04 - acc: 0.8401 - val_loss: 4.3270e-05 - val_acc: 0.9000\n",
      "Epoch 578/600\n",
      " - 1s - loss: 3.4582e-04 - acc: 0.8413 - val_loss: 3.2274e-05 - val_acc: 0.9222\n",
      "Epoch 579/600\n",
      " - 1s - loss: 3.2098e-04 - acc: 0.8397 - val_loss: 8.2235e-06 - val_acc: 0.9222\n",
      "Epoch 580/600\n",
      " - 1s - loss: 3.0847e-04 - acc: 0.8397 - val_loss: 1.5398e-05 - val_acc: 0.9278\n",
      "Epoch 581/600\n",
      " - 1s - loss: 3.0885e-04 - acc: 0.8437 - val_loss: 6.7454e-05 - val_acc: 0.9167\n",
      "Epoch 582/600\n",
      " - 1s - loss: 3.4569e-04 - acc: 0.8440 - val_loss: 5.4800e-05 - val_acc: 0.9000\n",
      "Epoch 583/600\n",
      " - 1s - loss: 3.3907e-04 - acc: 0.8429 - val_loss: 1.4033e-05 - val_acc: 0.9111\n",
      "Epoch 584/600\n",
      " - 1s - loss: 3.1019e-04 - acc: 0.8405 - val_loss: 4.9122e-05 - val_acc: 0.9111\n",
      "Epoch 585/600\n",
      " - 1s - loss: 3.1875e-04 - acc: 0.8444 - val_loss: 8.1141e-05 - val_acc: 0.9056\n",
      "Epoch 586/600\n",
      " - 1s - loss: 3.8591e-04 - acc: 0.8325 - val_loss: 9.7472e-05 - val_acc: 0.9167\n",
      "Epoch 587/600\n",
      " - 1s - loss: 3.6280e-04 - acc: 0.8425 - val_loss: 3.0745e-05 - val_acc: 0.9056\n",
      "Epoch 588/600\n",
      " - 1s - loss: 3.1693e-04 - acc: 0.8349 - val_loss: 1.6219e-05 - val_acc: 0.9111\n",
      "Epoch 589/600\n",
      " - 1s - loss: 3.0236e-04 - acc: 0.8389 - val_loss: 9.9671e-05 - val_acc: 0.8944\n",
      "Epoch 590/600\n",
      " - 1s - loss: 3.8155e-04 - acc: 0.8528 - val_loss: 1.2408e-04 - val_acc: 0.8889\n",
      "Epoch 591/600\n",
      " - 1s - loss: 4.1861e-04 - acc: 0.8341 - val_loss: 5.6491e-05 - val_acc: 0.9000\n",
      "Epoch 592/600\n",
      " - 1s - loss: 3.2702e-04 - acc: 0.8357 - val_loss: 2.1125e-05 - val_acc: 0.9167\n",
      "Epoch 593/600\n",
      " - 1s - loss: 2.9492e-04 - acc: 0.8484 - val_loss: 5.7457e-05 - val_acc: 0.9111\n",
      "Epoch 594/600\n",
      " - 1s - loss: 3.4271e-04 - acc: 0.8456 - val_loss: 9.7600e-05 - val_acc: 0.9167\n",
      "Epoch 595/600\n",
      " - 1s - loss: 3.6658e-04 - acc: 0.8433 - val_loss: 2.9618e-05 - val_acc: 0.9000\n",
      "Epoch 596/600\n",
      " - 1s - loss: 3.1025e-04 - acc: 0.8345 - val_loss: 2.4783e-05 - val_acc: 0.9111\n",
      "Epoch 597/600\n",
      " - 1s - loss: 3.0024e-04 - acc: 0.8480 - val_loss: 2.0639e-05 - val_acc: 0.9056\n",
      "Epoch 598/600\n",
      " - 1s - loss: 2.9904e-04 - acc: 0.8417 - val_loss: 6.4193e-05 - val_acc: 0.9056\n",
      "Epoch 599/600\n",
      " - 1s - loss: 3.4854e-04 - acc: 0.8329 - val_loss: 8.9031e-05 - val_acc: 0.9056\n",
      "Epoch 600/600\n",
      " - 1s - loss: 3.5664e-04 - acc: 0.8484 - val_loss: 5.8410e-05 - val_acc: 0.9000\n",
      "Saved model to disk\n",
      "Loaded model from disk\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 600\n",
    "num_unit = 240\n",
    "\n",
    "Test_LSTM('Stateless', 'Inspace', 1, 24, 48, num_unit, num_epoch, 2520, 360, 720, \"Adam\",'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
